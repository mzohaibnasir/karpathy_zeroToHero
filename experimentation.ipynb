{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 512]),\n",
       " tensor([[[ 0.2098, -0.3947, -0.2909,  ..., -0.2254,  1.2406, -1.0059],\n",
       "          [-0.1649, -0.2299, -0.4710,  ..., -0.8700, -0.7264, -0.6422],\n",
       "          [-0.0617, -0.4992, -0.2882,  ...,  0.2205,  1.1892,  1.2710],\n",
       "          [-2.2563, -1.6153,  0.7384,  ..., -1.1690, -0.6215,  0.0888],\n",
       "          [-2.2563, -1.6153,  0.7384,  ..., -1.1690, -0.6215,  0.0888]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we;ll be building input embeddings\n",
    "# allows to convert token into embedding of dim 1x52  : token -> input ID(position in vocab) ->embedding\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            d_model (int): dim of vector\n",
    "            vocab_size (int): # of words in vocab\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "# Create an example input tensor (batch size 1, sequence length 5, embedding dimension 20)\n",
    "batch_of_sentences = torch.tensor([[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "print(batch_of_sentences.shape)\n",
    "\n",
    "\n",
    "# Pass through the embedding layer\n",
    "# The forward method is called automatically when you use the instance like a function.\n",
    "embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "embedded_sentences.shape, embedded_sentences  # (batch, seq_len, embedding dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "d_model = 6\n",
    "nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor([[[ 0.2098, -0.3947, -0.2909,  ..., -0.2254,  1.2406, -1.0059],\n",
      "         [-0.1649, -0.2299, -0.4710,  ..., -0.8700, -0.7264, -0.6422],\n",
      "         [-0.0617, -0.4992, -0.2882,  ...,  0.2205,  1.1892,  1.2710],\n",
      "         [-2.2563, -1.6153,  0.7384,  ..., -1.1690, -0.6215,  0.0888],\n",
      "         [-2.2563, -1.6153,  0.7384,  ..., -1.1690, -0.6215,  0.0888]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "input shape torch.Size([1, 5, 512])\n",
      "positional_encoded shape torch.Size([1, 5, 512])\n",
      "tensor([[[ 0.4196,  1.2105, -0.5818,  ...,  1.5491,  2.4813, -0.0000],\n",
      "         [ 1.3532,  0.6207,  0.7017,  ...,  0.0000, -1.4526,  0.7157],\n",
      "         [ 1.6952, -1.8307,  1.2964,  ...,  2.4411,  2.3788,  0.0000],\n",
      "         [-4.2303, -5.2106,  1.9669,  ..., -0.3380, -0.0000,  2.1776],\n",
      "         [-6.0262, -0.0000,  0.1624,  ..., -0.0000, -1.2421,  2.1776]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "                Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "        order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "        tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "        bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "        as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "        learned and fixed [9].\n",
    "        In this work, we use sine and cosine functions of different frequencies:\n",
    "            `PE(pos,2i) = sin(pos/(10000)**2i/dmodel)`\n",
    "            `PE(pos,2i+1) = cos(pos/(10000)**2i/dmodel)`\n",
    "        where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "        corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "        chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "        relative positions, since for any fixed offset k, P E(pos+k) can be represented as a linear function of\n",
    "        PE(pos).\n",
    "\n",
    "        Keyword arguments:\n",
    "        dropout -- to make model less overfit\n",
    "        seq_len -- Specifies the maximum length of sequence that the model can handle. This helps determine the scale and range of the positional encodings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encodeing shape: seq_len X d_model i.e. each token will be represented (1*d_model) vector\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        formula :`PE(pos,2i) = cos(pos/(10000)**2i/dmodel) for i=1,3,5, ...and `PE(pos,2i) = sin(pos/(10000)**2i/dmodel) for i=2,4,6, ...and `\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #  Create a model of shape (seq_len , d_model)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #  create a vector of shape(seq_len,1) to represent position of word in sequence\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len,1)  # pos in formula\n",
    "        # create denominator of formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # apply sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # apply cos to odd positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # now we need to add batch dimension to these sentences so we can apply it to whole sentences, so to all the batch of sentence, because weill have batch of sentences.\n",
    "        # adding batch dim\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # register this tensor in buffer of module  .. it is done for the tensor that you want to keep inside the module, not as a lerarned parameter but you want it to be saved when you save the file of the model\n",
    "        # you should register it as a buffer. this way the tensor would be saved in file along with state of model\n",
    "        self.register_buffer(\"pe\", pe)  # This is typically used to register a buffer that should not to be considered a model parameter.\n",
    "        \"\"\"\n",
    "        Say you have a linear layer nn.Linear. You already have weight and bias parameters. But if you need a new parameter you use register_parameter() to register a new named parameter that is a tensor.\n",
    "        When you register a new parameter it will appear inside the module.parameters() iterator, but when you register a buffer it will not.\n",
    "        The difference:\n",
    "        Buffers are named tensors that do not update gradients at every step, like parameters. For buffers, you create your custom logic (fully up to you).\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we need to add positional encoding to every token/word inside sequence/sentence\n",
    "        \"\"\"\n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad_(False)  # x:token and pe is positional encoding  # because we dont want to learn pe because these are fixed\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "positional = PositionalEncoding(d_model=512, seq_len=5, dropout=0.5)\n",
    "\n",
    "# Create an example input tensor (batch size , sequence length , embedding dimension )\n",
    "\n",
    "# Apply positional encoding\n",
    "positional_encoded = positional(embedded_sentences)\n",
    "print(\"input \", embedded_sentences)\n",
    "\n",
    "print(\"input shape\", embedded_sentences.shape)\n",
    "print(\"positional_encoded shape\", positional_encoded.shape)\n",
    "\n",
    "print(positional_encoded)  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 register_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule()\n",
      "Buffer tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Another buffer tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        # Register a buffer tensor with zeros\n",
    "        self.register_buffer(\"buffer_tensor\", torch.zeros(3, 3))\n",
    "\n",
    "        # Register another buffer tensor with specific values\n",
    "        data = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "        self.register_buffer(\"another_buffer\", data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the buffer tensors in the forward pass\n",
    "        output = x + self.buffer_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of MyModule\n",
    "model = MyModule()\n",
    "\n",
    "# Print the module to see its structure\n",
    "print(model)\n",
    "\n",
    "# Accessing the buffer tensors\n",
    "print(\"Buffer tensor:\")\n",
    "print(model.buffer_tensor)\n",
    "\n",
    "print(\"\\nAnother buffer tensor:\")\n",
    "print(model.another_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add & Norm - layer normalization\n",
    "\n",
    "for each item in the batch, calculte mean & var, and normalize each item so that each has mean=0, and var of 1(z-standardization), Beta and Gamma are also learnt to minimize the data flactuation as having values between - and 1 might be too restrictive.\n",
    "\n",
    "new xj = (xj -meanj) / math.sqrt(var\\*\\*2 + epsilon)\n",
    "\n",
    "simplified version: `x = α * (x - μ) / (σ + ε) + β`\n",
    "\n",
    "gamma(multiplication) and beta(addition) will be learnt after this. epsilon is for numericalsatability as if denominator gets very small, overall number would be difficult to manage percision wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0627,  0.4125, -0.6643,  ...,  0.6159,  1.1759, -0.3148],\n",
       "         [ 0.4697,  0.0163,  0.0664,  ..., -0.3680, -1.2672,  0.0750],\n",
       "         [ 0.7076, -1.4773,  0.4605,  ...,  1.1698,  1.1312, -0.3429],\n",
       "         [-2.9017, -3.4909,  0.8233,  ..., -0.5621, -0.3590,  0.9499],\n",
       "         [-3.5371, -0.3253, -0.2388,  ..., -0.3253, -0.9873,  0.8353]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # gamma  # mulltiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # print(\"mean shape\", mean.shape, mean)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "ln = LayerNormalization()\n",
    "\n",
    "# print(\"Before normalization:\")\n",
    "# print(positional_encoded)\n",
    "\n",
    "normalized = ln(positional_encoded)\n",
    "# print(\"After normalization:\")\n",
    "print(normalized.shape)\n",
    "normalized  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. feed forward block\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "`FFN(x) = max(0, xW1 + b1)W2 + b2 (2)` # two lyers with ReLu in between\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feedforwardblock:\n",
      "torch.Size([1, 5, 512]) tensor([[[-0.0627,  0.4125, -0.6643,  ...,  0.6159,  1.1759, -0.3148],\n",
      "         [ 0.4697,  0.0163,  0.0664,  ..., -0.3680, -1.2672,  0.0750],\n",
      "         [ 0.7076, -1.4773,  0.4605,  ...,  1.1698,  1.1312, -0.3429],\n",
      "         [-2.9017, -3.4909,  0.8233,  ..., -0.5621, -0.3590,  0.9499],\n",
      "         [-3.5371, -0.3253, -0.2388,  ..., -0.3253, -0.9873,  0.8353]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5375,  0.0354, -0.0791,  ...,  0.5518,  0.1221, -0.3908],\n",
       "         [-0.7616,  0.3545,  0.2357,  ..., -0.1363,  0.6434,  0.5094],\n",
       "         [-0.4188, -0.3181,  0.4288,  ...,  0.1878, -0.2010, -0.0597],\n",
       "         [-0.1479,  0.2047,  0.1629,  ...,  0.6414, -0.7571, -0.3854],\n",
       "         [ 0.2172, -0.0781, -0.1686,  ...,  0.1476, -0.6196,  0.2035]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)  # first layer: w1,b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)  # second layer: w2,b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input:(batch, seq_len, d_model)\n",
    "\n",
    "        # after first layer: (batch, seq_len, d_ff)\n",
    "\n",
    "        # after second layer: (batch, seq_len, d_model)\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "feedforwardblock = FeedForwardBlock(d_model=512, d_ff=2048, dropout=0.5)\n",
    "\n",
    "print(\"Before feedforwardblock:\")\n",
    "print(normalized.shape, normalized)\n",
    "\n",
    "feedforwarded = feedforwardblock(normalized)\n",
    "# print(\"After normalization:\")\n",
    "print(feedforwarded.shape)\n",
    "feedforwarded  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-head attention\n",
    "\n",
    "takes input:(seq_len, d_model) of encoder and uses it three times k:key, q:query, v:values. then we multiply these matrices with Wk, Wq and Wv respectively. resulting in K',Q',V' of same(seq_len, d_model) dim. Now,split each of K', Q' and V' into h parts along d_model(embedding) dim where h is number of head. So that each head will have access to full sentence but different part of embedding of each token.\n",
    "\n",
    "Now, apply following formulas to each head which will result into h matrices of `(seq_len, d_k)` dims where `d_k` = `d_model/h`\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW^Q_i, K W^K_i, V W^V_i)\n",
    "$$\n",
    "\n",
    "Now concatenate all heads,\n",
    "\n",
    "$$\n",
    "\\text{MultiHead(Q, K, V)} = \\text{Concatenate}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^o\n",
    "$$\n",
    "\n",
    "![alt text](02_transformer/MHA.png)\n",
    "\n",
    "W^o is of `(seq, h*d_v)` shape where `d_v = d_k`\n",
    "\n",
    "and resultant MH-A is `(seq_len, d_model)` same as input\n",
    "\n",
    "But we also have to consider batch_dim for dealing with multiple sentences; the above intition works for single sentence.\n",
    "\n",
    "`SO WE WILL CONSIDER BATCH DIMENSION.`\n",
    "\n",
    "---\n",
    "\n",
    "### MASK\n",
    "\n",
    "before applying multiplying with V meaning\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "we get a scaled dot profuct of q and k, its (seq_len, seq_len) matrix. this shows interaction of each words with each other word.\n",
    "\n",
    "If we dont want some words to interact with other words, we basically replace there attention score(before applying softmax) with very small value, which means after softmax these values will become zero,so basically we hide attention between those two words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = dropout\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        # define weight matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # wq\n",
    "        self.wk = nn.Linear(d_model, d_model)  # wk\n",
    "        self.wv = nn.Linear(d_model, d_model)  # wv\n",
    "\n",
    "        # output matrix Wo (h*dv, d_model) where dv = dk\n",
    "        self.wo = nn.Linear(d_model, d_model)  # wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod  # so we could cal fn wothout specifying class instance\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]  # last dim of query/key/value\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  # transpose(-2, -1): transpose last two dims\n",
    "\n",
    "        # apply mask: just replace values you want to mask with very small values\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9)  # replace all values where mask==0 (conidtion is true) with -1e9\n",
    "\n",
    "        # applying softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len,seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        query = self.wq(q)\n",
    "        key = self.wq(k)\n",
    "        value = self.wq(v)\n",
    "\n",
    "        # splitting\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        # we moved h dimension because we want each head to consider (seq_len, d_k)\n",
    "        # each head considers full sentence but smaller embedding\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch, h, seq_len, d_k)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query=query, key=key, value=value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual/skip connection\n",
    "\n",
    "between add & Norm and previous layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):  # sublayer:previous layer\n",
    "        \"\"\"take x and combine with output of next layer\"\"\"\n",
    "\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoderblock\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n",
    "\n",
    "it will contain one multi-head attention, two Add&Norm, one Feed forward block and two residual connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        src_mask: mask we want to apply to input of encoder. we need this to hide interaction of padding word with other words.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # multihead attention within\n",
    "\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        The lambda is used because self_attention_block needs four arguments (query, key, value, mask) \n",
    "        but ResidualConnection expects a function that takes only one argument.\n",
    "        The lambda allows us to create a function that takes one argument x and expands it to the required four arguments, including the src_mask.\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "is made up of many encoder\n",
    "\n",
    "Each encoder block is repeated Nx times\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "        self.norm = LayerNormalization()  # at end\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # apply on layer after another # order matters\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 15-50-17.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output embeddings\n",
    "\n",
    "output embeddings are same as input embeddings, so weill just intialize it twice\n",
    "\n",
    "masked attention is some what same as self attenntion because of 3 same inputs while Mulihead attention block is actually cross attension bea=cause key and value are cping from encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        # we have three residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(3)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Args:\n",
    "            x: input of decoder\n",
    "            src_mask: mask applied to encoder\n",
    "            tgt_mask: target mask applied to decoder\n",
    "\n",
    "\n",
    "\n",
    "        src_mask and tgt_mask because we are dealing with language transalation. SO, source language is English and target language is italian\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # i. masked multihead attention: first residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))  # tgt_mask:becasue its decoder\n",
    "\n",
    "        # ii. cross attention: second residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        x = self.residual_connections[2](x, feedforwardblock)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"build decoder which is n times DecoderBlock one after anotherjust we did for encoder\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): whic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            # each layer is a decoderblock\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection/linear layer\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-23 10-50-34.png>)\n",
    "\n",
    "output of multihead attention is (seq_len, d_model)\n",
    "\n",
    "However we want to these words back into vocabularly which convert embedding to position in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_moel: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        this is a linear layer that is converting from d_model to vocab_size\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch seq_len, vocab_size)\n",
    "        # The purpose of applying softmax is to convert the raw output of the linear layer into a probability distribution over the vocabulary.\n",
    "        #  we will also apply softmax, specifically log_softmax for mathematiacal stability\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noManEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
