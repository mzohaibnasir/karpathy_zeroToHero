{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 512]),\n",
       " tensor([[[ 0.2822,  0.9554,  0.8966,  ...,  0.0817,  0.6547, -1.7688],\n",
       "          [-1.2638,  0.3615, -1.7018,  ..., -0.5217,  0.1918,  0.7739],\n",
       "          [ 0.5932, -3.0024, -1.2747,  ...,  2.1137, -1.7159, -0.7693],\n",
       "          [-0.1027, -1.1518, -0.9854,  ..., -0.7678, -0.7317, -0.1506],\n",
       "          [-0.1027, -1.1518, -0.9854,  ..., -0.7678, -0.7317, -0.1506]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we;ll be building input embeddings\n",
    "# allows to convert token into embedding of dim 1x52  : token -> input ID(position in vocab) ->embedding\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            d_model (int): dim of vector\n",
    "            vocab_size (int): # of words in vocab\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "# Create an example input tensor (batch size 1, sequence length 5, embedding dimension 20)\n",
    "batch_of_sentences = torch.tensor([[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "print(batch_of_sentences.shape)\n",
    "\n",
    "\n",
    "# Pass through the embedding layer\n",
    "# The forward method is called automatically when you use the instance like a function.\n",
    "embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "embedded_sentences.shape, embedded_sentences  # (batch, seq_len, embedding dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "d_model = 6\n",
    "nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor([[[ 0.2822,  0.9554,  0.8966,  ...,  0.0817,  0.6547, -1.7688],\n",
      "         [-1.2638,  0.3615, -1.7018,  ..., -0.5217,  0.1918,  0.7739],\n",
      "         [ 0.5932, -3.0024, -1.2747,  ...,  2.1137, -1.7159, -0.7693],\n",
      "         [-0.1027, -1.1518, -0.9854,  ..., -0.7678, -0.7317, -0.1506],\n",
      "         [-0.1027, -1.1518, -0.9854,  ..., -0.7678, -0.7317, -0.1506]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "input shape torch.Size([1, 5, 512])\n",
      "positional_encoded shape torch.Size([1, 5, 512])\n",
      "tensor([[[ 0.0000,  0.0000,  1.7933,  ...,  0.0000,  0.0000, -0.0000],\n",
      "         [-0.8447,  0.0000, -0.0000,  ...,  0.0000,  0.3838,  3.5479],\n",
      "         [ 0.0000, -0.0000, -0.6766,  ...,  6.2274, -0.0000,  0.4614],\n",
      "         [ 0.0000, -4.2836, -1.4807,  ...,  0.0000, -0.0000,  1.6989],\n",
      "         [-0.0000, -0.0000, -3.2852,  ...,  0.0000, -1.4627,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "                Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "        order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "        tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "        bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "        as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "        learned and fixed [9].\n",
    "        In this work, we use sine and cosine functions of different frequencies:\n",
    "            `PE(pos,2i) = sin(pos/(10000)**2i/dmodel)`\n",
    "            `PE(pos,2i+1) = cos(pos/(10000)**2i/dmodel)`\n",
    "        where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "        corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "        chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "        relative positions, since for any fixed offset k, P E(pos+k) can be represented as a linear function of\n",
    "        PE(pos).\n",
    "\n",
    "        Keyword arguments:\n",
    "        dropout -- to make model less overfit\n",
    "        seq_len -- Specifies the maximum length of sequence that the model can handle. This helps determine the scale and range of the positional encodings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encodeing shape: seq_len X d_model i.e. each token will be represented (1*d_model) vector\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        formula :`PE(pos,2i) = cos(pos/(10000)**2i/dmodel) for i=1,3,5, ...and `PE(pos,2i) = sin(pos/(10000)**2i/dmodel) for i=2,4,6, ...and `\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #  Create a model of shape (seq_len , d_model)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #  create a vector of shape(seq_len,1) to represent position of word in sequence\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len,1)  # pos in formula\n",
    "        # create denominator of formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # apply sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # apply cos to odd positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # now we need to add batch dimension to these sentences so we can apply it to whole sentences, so to all the batch of sentence, because weill have batch of sentences.\n",
    "        # adding batch dim\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # register this tensor in buffer of module  .. it is done for the tensor that you want to keep inside the module, not as a lerarned parameter but you want it to be saved when you save the file of the model\n",
    "        # you should register it as a buffer. this way the tensor would be saved in file along with state of model\n",
    "        self.register_buffer(\"pe\", pe)  # This is typically used to register a buffer that should not to be considered a model parameter.\n",
    "        \"\"\"\n",
    "        Say you have a linear layer nn.Linear. You already have weight and bias parameters. But if you need a new parameter you use register_parameter() to register a new named parameter that is a tensor.\n",
    "        When you register a new parameter it will appear inside the module.parameters() iterator, but when you register a buffer it will not.\n",
    "        The difference:\n",
    "        Buffers are named tensors that do not update gradients at every step, like parameters. For buffers, you create your custom logic (fully up to you).\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we need to add positional encoding to every token/word inside sequence/sentence\n",
    "        \"\"\"\n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad_(False)  # x:token and pe is positional encoding  # because we dont want to learn pe because these are fixed\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "positional = PositionalEncoding(d_model=512, seq_len=5, dropout=0.5)\n",
    "\n",
    "# Create an example input tensor (batch size , sequence length , embedding dimension )\n",
    "\n",
    "# Apply positional encoding\n",
    "positional_encoded = positional(embedded_sentences)\n",
    "print(\"input \", embedded_sentences)\n",
    "\n",
    "print(\"input shape\", embedded_sentences.shape)\n",
    "print(\"positional_encoded shape\", positional_encoded.shape)\n",
    "\n",
    "print(positional_encoded)  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 register_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule()\n",
      "Buffer tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Another buffer tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        # Register a buffer tensor with zeros\n",
    "        self.register_buffer(\"buffer_tensor\", torch.zeros(3, 3))\n",
    "\n",
    "        # Register another buffer tensor with specific values\n",
    "        data = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "        self.register_buffer(\"another_buffer\", data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the buffer tensors in the forward pass\n",
    "        output = x + self.buffer_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of MyModule\n",
    "model = MyModule()\n",
    "\n",
    "# Print the module to see its structure\n",
    "print(model)\n",
    "\n",
    "# Accessing the buffer tensors\n",
    "print(\"Buffer tensor:\")\n",
    "print(model.buffer_tensor)\n",
    "\n",
    "print(\"\\nAnother buffer tensor:\")\n",
    "print(model.another_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add & Norm - layer normalization\n",
    "\n",
    "for each item in the batch, calculte mean & var, and normalize each item so that each has mean=0, and var of 1(z-standardization), Beta and Gamma are also learnt to minimize the data flactuation as having values between - and 1 might be too restrictive.\n",
    "\n",
    "new xj = (xj -meanj) / math.sqrt(var\\*\\*2 + epsilon)\n",
    "\n",
    "simplified version: `x = α * (x - μ) / (σ + ε) + β`\n",
    "\n",
    "gamma(multiplication) and beta(addition) will be learnt after this. epsilon is for numericalsatability as if denominator gets very small, overall number would be difficult to manage percision wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2850, -0.2850,  0.8002,  ..., -0.2850, -0.2850, -0.2850],\n",
       "         [-0.8461, -0.3400, -0.3400,  ..., -0.3400, -0.1101,  1.7854],\n",
       "         [-0.4269, -0.4269, -0.8135,  ...,  3.1311, -0.4269, -0.1632],\n",
       "         [-0.3082, -2.7764, -1.1614,  ..., -0.3082, -0.3082,  0.6706],\n",
       "         [-0.2897, -0.2897, -2.3489,  ..., -0.2897, -1.2065, -0.2897]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # gamma  # mulltiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # print(\"mean shape\", mean.shape, mean)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "ln = LayerNormalization()\n",
    "\n",
    "# print(\"Before normalization:\")\n",
    "# print(positional_encoded)\n",
    "\n",
    "normalized = ln(positional_encoded)\n",
    "# print(\"After normalization:\")\n",
    "print(normalized.shape)\n",
    "normalized  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. feed forward block\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "`FFN(x) = max(0, xW1 + b1)W2 + b2 (2)` # two lyers with ReLu in between\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feedforwardblock:\n",
      "torch.Size([1, 5, 512]) tensor([[[-0.2850, -0.2850,  0.8002,  ..., -0.2850, -0.2850, -0.2850],\n",
      "         [-0.8461, -0.3400, -0.3400,  ..., -0.3400, -0.1101,  1.7854],\n",
      "         [-0.4269, -0.4269, -0.8135,  ...,  3.1311, -0.4269, -0.1632],\n",
      "         [-0.3082, -2.7764, -1.1614,  ..., -0.3082, -0.3082,  0.6706],\n",
      "         [-0.2897, -0.2897, -2.3489,  ..., -0.2897, -1.2065, -0.2897]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2137,  0.1589, -0.6476,  ..., -0.2574, -0.0153,  0.0769],\n",
       "         [ 0.2595, -0.7242,  0.3388,  ..., -0.2164, -0.6109,  0.1730],\n",
       "         [-0.2690,  0.1501, -0.3358,  ..., -0.0898, -0.1832,  0.4869],\n",
       "         [ 0.4717,  0.0390, -0.1872,  ...,  0.0208, -0.1170,  0.4581],\n",
       "         [ 0.7835, -0.5012,  0.4707,  ...,  0.1566, -0.1110,  0.4808]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)  # first layer: w1,b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)  # second layer: w2,b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input:(batch, seq_len, d_model)\n",
    "\n",
    "        # after first layer: (batch, seq_len, d_ff)\n",
    "\n",
    "        # after second layer: (batch, seq_len, d_model)\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "feedforwardblock = FeedForwardBlock(d_model=512, d_ff=2048, dropout=0.5)\n",
    "\n",
    "print(\"Before feedforwardblock:\")\n",
    "print(normalized.shape, normalized)\n",
    "\n",
    "feedforwarded = feedforwardblock(normalized)\n",
    "# print(\"After normalization:\")\n",
    "print(feedforwarded.shape)\n",
    "feedforwarded  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-head attention\n",
    "\n",
    "takes input:(seq_len, d_model) of encoder and uses it three times k:key, q:query, v:values. then we multiply these matrices with Wk, Wq and Wv respectively. resulting in K',Q',V' of same(seq_len, d_model) dim. Now,split each of K', Q' and V' into h parts along d_model(embedding) dim where h is number of head. So that each head will have access to full sentence but different part of embedding of each token.\n",
    "\n",
    "Now, apply following formulas to each head which will result into h matrices of `(seq_len, d_k)` dims where `d_k` = `d_model/h`\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW^Q_i, K W^K_i, V W^V_i)\n",
    "$$\n",
    "\n",
    "Now concatenate all heads,\n",
    "\n",
    "$$\n",
    "\\text{MultiHead(Q, K, V)} = \\text{Concatenate}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^o\n",
    "$$\n",
    "\n",
    "![alt text](02_transformer/MHA.png)\n",
    "\n",
    "W^o is of `(seq, h*d_v)` shape where `d_v = d_k`\n",
    "\n",
    "and resultant MH-A is `(seq_len, d_model)` same as input\n",
    "\n",
    "But we also have to consider batch_dim for dealing with multiple sentences; the above intition works for single sentence.\n",
    "\n",
    "`SO WE WILL CONSIDER BATCH DIMENSION.`\n",
    "\n",
    "---\n",
    "\n",
    "### MASK\n",
    "\n",
    "before applying multiplying with V meaning\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "we get a scaled dot profuct of q and k, its (seq_len, seq_len) matrix. this shows interaction of each words with each other word.\n",
    "\n",
    "If we dont want some words to interact with other words, we basically replace there attention score(before applying softmax) with very small value, which means after softmax these values will become zero,so basically we hide attention between those two words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = dropout\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        # define weight matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # wq\n",
    "        self.wk = nn.Linear(d_model, d_model)  # wk\n",
    "        self.wv = nn.Linear(d_model, d_model)  # wv\n",
    "\n",
    "        # output matrix Wo (h*dv, d_model) where dv = dk\n",
    "        self.wo = nn.Linear(d_model, d_model)  # wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod  # so we could cal fn wothout specifying class instance\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]  # last dim of query/key/value\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  # transpose(-2, -1): transpose last two dims\n",
    "\n",
    "        # apply mask: just replace values you want to mask with very small values\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9)  # replace all values where mask==0 (conidtion is true) with -1e9\n",
    "\n",
    "        # applying softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len,seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        query = self.wq(q)\n",
    "        key = self.wq(k)\n",
    "        value = self.wq(v)\n",
    "\n",
    "        # splitting\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        # we moved h dimension because we want each head to consider (seq_len, d_k)\n",
    "        # each head considers full sentence but smaller embedding\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch, h, seq_len, d_k)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query=query, key=key, value=value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual/skip connection\n",
    "\n",
    "between add & Norm and previous layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):  # sublayer:previous layer\n",
    "        \"\"\"take x and combine with output of next layer\"\"\"\n",
    "\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoderblock\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n",
    "\n",
    "it will contain one multi-head attention, two Add&Norm, one Feed forward block and two residual connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        src_mask: mask we want to apply to input of encoder. we need this to hide interaction of padding word with other words.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # multihead attention within\n",
    "\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        The lambda is used because self_attention_block needs four arguments (query, key, value, mask) \n",
    "        but ResidualConnection expects a function that takes only one argument.\n",
    "        The lambda allows us to create a function that takes one argument x and expands it to the required four arguments, including the src_mask.\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "is made up of many encoder\n",
    "\n",
    "Each encoder block is repeated Nx times\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "        self.norm = LayerNormalization()  # at end\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # apply on layer after another # order matters\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 15-50-17.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output embeddings\n",
    "\n",
    "output embeddings are same as input embeddings, so weill just intialize it twice\n",
    "\n",
    "masked attention is some what same as self attenntion because of 3 same inputs while Mulihead attention block is actually cross attension bea=cause key and value are cping from encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        # we have three residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(3)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Args:\n",
    "            x: input of decoder\n",
    "            src_mask: mask applied to encoder\n",
    "            tgt_mask: target mask applied to decoder\n",
    "\n",
    "\n",
    "\n",
    "        src_mask and tgt_mask because we are dealing with language transalation. SO, source language is English and target language is italian\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # i. masked multihead attention: first residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))  # tgt_mask:becasue its decoder\n",
    "\n",
    "        # ii. cross attention: second residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        x = self.residual_connections[2](x, feedforwardblock)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"build decoder which is n times DecoderBlock one after anotherjust we did for encoder\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): whic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            # each layer is a decoderblock\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection/linear layer\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-23 10-50-34.png>)\n",
    "\n",
    "output of multihead attention is (seq_len, d_model)\n",
    "\n",
    "However we want to these words back into vocabularly which convert embedding to position in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_moel: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        this is a linear layer that is converting from d_model to vocab_size\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch seq_len, vocab_size)\n",
    "        # The purpose of applying softmax is to convert the raw output of the linear layer into a probability distribution over the vocabulary.\n",
    "        #  we will also apply softmax, specifically log_softmax for mathematiacal stability\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        \"\"\"\n",
    "        we need source embedding and target embedding because we are dealing with multiple languages\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # three methods, one to encoder, one to decode and one to project\n",
    "    # Not creating single forward method because we can reuse output of encoder and to also visualize the attention\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            src (_type_): src of language\n",
    "            src_mask (_type_): source mask\n",
    "        \"\"\"\n",
    "\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        src = self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_ouput, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_ouput, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    \"\"\"sumary_line\n",
    "\n",
    "    we need vocab size of src and tgt so get info about how many vectors to be created\n",
    "    Keyword arguments:\n",
    "\n",
    "    N: number of input layers i.e. number of enccoder blocks and number of decoder blocks\n",
    "    h: # of heads\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"strcuture will be same across all tasks\"\"\"\n",
    "\n",
    "    # create embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # positional encoding layers\n",
    "    # one encoding layer wold be enough\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "    # create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = feedforwardblock(d_model, d_ff, dropout)\n",
    "\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    # create encoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = feedforwardblock(d_model, d_ff, dropout)\n",
    "\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # now create encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # create projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # create transfromer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # initilize parameter to make trainig faster so they dont just strat with random values\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# engIt - task\n",
    "\n",
    "1. Download dataset: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it\n",
    "2. build tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer: create sentence into tokens. h=there are manyh tokenizers like BPE, subword-level,, word level, etc\n",
    "# we'll be creting word-level tokenizer i.e. split by space\n",
    "# so tokenizer builds vocab and maps tokens to index\n",
    "# there would be special tokens too for paddings, start of sentence, end of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train,py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohaib/anaconda3/envs/noManEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "# class that will train tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace  # to split words according to whitespace\n",
    "\n",
    "from pathlib import Path  # to assist in creating absolutes path using relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    # pasrsing each item which is a pair in dataset # (english, italian)\n",
    "    for item in ds:\n",
    "        yield item[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buils the tokeizer\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    \"\"\"\n",
    "    Building the tokenizer\n",
    "\n",
    "    Keyword arguments:\n",
    "    config -- config of our model\n",
    "    ds -- dataset\n",
    "    lang -- lang to build tokeizer for\n",
    "    \"\"\"\n",
    "\n",
    "    # file to save this tokenizer\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))  # mean we can change\n",
    "    #'../tokenizers/tokenizer_{0}.format(lang).json' same as f\"'../tokenizers/tokenizer_{lang}'\n",
    "\n",
    "    # so if tokenizer ddoesn't exists we create it\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()  # split by whitespces\n",
    "        #  now training tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)  # for a word to appear in vocab it must have min frequency of 2\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        print(tokenizer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and get tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "    # build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # keep 10% for val, 90 for trainig ... hf dst has single split so we'll be splitting manually\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noManEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
