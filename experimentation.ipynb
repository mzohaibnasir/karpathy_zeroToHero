{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 512]),\n",
       " tensor([[[ 1.5218,  1.0482, -0.7121,  ..., -0.3793,  0.0541, -1.2825],\n",
       "          [ 2.1450, -0.1365,  0.0885,  ..., -0.2892,  0.6560,  0.0397],\n",
       "          [ 0.9029, -0.7957,  0.1330,  ...,  0.5349, -0.0488,  0.5661],\n",
       "          [ 0.2489, -0.5250, -1.3759,  ..., -1.6443,  0.1639,  1.1627],\n",
       "          [ 0.2489, -0.5250, -1.3759,  ..., -1.6443,  0.1639,  1.1627]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we;ll be building input embeddings\n",
    "# allows to convert token into embedding of dim 1x52  : token -> input ID(position in vocab) ->embedding\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            d_model (int): dim of vector\n",
    "            vocab_size (int): # of words in vocab\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "# Create an example input tensor (batch size 1, sequence length 5, embedding dimension 20)\n",
    "batch_of_sentences = torch.tensor([[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "print(batch_of_sentences.shape)\n",
    "\n",
    "\n",
    "# Pass through the embedding layer\n",
    "# The forward method is called automatically when you use the instance like a function.\n",
    "embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "embedded_sentences.shape, embedded_sentences  # (batch, seq_len, embedding dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "d_model = 6\n",
    "nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor([[[ 1.5218,  1.0482, -0.7121,  ..., -0.3793,  0.0541, -1.2825],\n",
      "         [ 2.1450, -0.1365,  0.0885,  ..., -0.2892,  0.6560,  0.0397],\n",
      "         [ 0.9029, -0.7957,  0.1330,  ...,  0.5349, -0.0488,  0.5661],\n",
      "         [ 0.2489, -0.5250, -1.3759,  ..., -1.6443,  0.1639,  1.1627],\n",
      "         [ 0.2489, -0.5250, -1.3759,  ..., -1.6443,  0.1639,  1.1627]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "input shape torch.Size([1, 5, 512])\n",
      "positional_encoded shape torch.Size([1, 5, 512])\n",
      "tensor([[[ 0.0000,  4.0964, -1.4242,  ...,  0.0000,  0.1082, -0.5650],\n",
      "         [ 0.0000,  0.0000,  1.8207,  ...,  1.4215,  0.0000,  2.0794],\n",
      "         [ 0.0000, -2.4237,  0.0000,  ...,  0.0000, -0.0971,  3.1322],\n",
      "         [ 0.0000, -3.0301, -2.2615,  ..., -0.0000,  0.3283,  0.0000],\n",
      "         [-0.0000, -2.3574, -0.0000,  ..., -0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "                Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "        order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "        tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "        bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "        as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "        learned and fixed [9].\n",
    "        In this work, we use sine and cosine functions of different frequencies:\n",
    "            `PE(pos,2i) = sin(pos/(10000)**2i/dmodel)`\n",
    "            `PE(pos,2i+1) = cos(pos/(10000)**2i/dmodel)`\n",
    "        where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "        corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "        chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "        relative positions, since for any fixed offset k, P E(pos+k) can be represented as a linear function of\n",
    "        PE(pos).\n",
    "\n",
    "        Keyword arguments:\n",
    "        dropout -- to make model less overfit\n",
    "        seq_len -- Specifies the maximum length of sequence that the model can handle. This helps determine the scale and range of the positional encodings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encodeing shape: seq_len X d_model i.e. each token will be represented (1*d_model) vector\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        formula :`PE(pos,2i) = cos(pos/(10000)**2i/dmodel) for i=1,3,5, ...and `PE(pos,2i) = sin(pos/(10000)**2i/dmodel) for i=2,4,6, ...and `\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #  Create a model of shape (seq_len , d_model)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #  create a vector of shape(seq_len,1) to represent position of word in sequence\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len,1)  # pos in formula\n",
    "        # create denominator of formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # apply sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # apply cos to odd positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # now we need to add batch dimension to these sentences so we can apply it to whole sentences, so to all the batch of sentence, because weill have batch of sentences.\n",
    "        # adding batch dim\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # register this tensor in buffer of module  .. it is done for the tensor that you want to keep inside the module, not as a lerarned parameter but you want it to be saved when you save the file of the model\n",
    "        # you should register it as a buffer. this way the tensor would be saved in file along with state of model\n",
    "        self.register_buffer(\"pe\", pe)  # This is typically used to register a buffer that should not to be considered a model parameter.\n",
    "        \"\"\"\n",
    "        Say you have a linear layer nn.Linear. You already have weight and bias parameters. But if you need a new parameter you use register_parameter() to register a new named parameter that is a tensor.\n",
    "        When you register a new parameter it will appear inside the module.parameters() iterator, but when you register a buffer it will not.\n",
    "        The difference:\n",
    "        Buffers are named tensors that do not update gradients at every step, like parameters. For buffers, you create your custom logic (fully up to you).\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we need to add positional encoding to every token/word inside sequence/sentence\n",
    "        \"\"\"\n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad_(False)  # x:token and pe is positional encoding  # because we dont want to learn pe because these are fixed\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "positional = PositionalEncoding(d_model=512, seq_len=5, dropout=0.5)\n",
    "\n",
    "# Create an example input tensor (batch size , sequence length , embedding dimension )\n",
    "\n",
    "# Apply positional encoding\n",
    "positional_encoded = positional(embedded_sentences)\n",
    "print(\"input \", embedded_sentences)\n",
    "\n",
    "print(\"input shape\", embedded_sentences.shape)\n",
    "print(\"positional_encoded shape\", positional_encoded.shape)\n",
    "\n",
    "print(positional_encoded)  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 register_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule()\n",
      "Buffer tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Another buffer tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        # Register a buffer tensor with zeros\n",
    "        self.register_buffer(\"buffer_tensor\", torch.zeros(3, 3))\n",
    "\n",
    "        # Register another buffer tensor with specific values\n",
    "        data = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "        self.register_buffer(\"another_buffer\", data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the buffer tensors in the forward pass\n",
    "        output = x + self.buffer_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of MyModule\n",
    "model = MyModule()\n",
    "\n",
    "# Print the module to see its structure\n",
    "print(model)\n",
    "\n",
    "# Accessing the buffer tensors\n",
    "print(\"Buffer tensor:\")\n",
    "print(model.buffer_tensor)\n",
    "\n",
    "print(\"\\nAnother buffer tensor:\")\n",
    "print(model.another_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add & Norm - layer normalization\n",
    "\n",
    "for each item in the batch, calculte mean & var, and normalize each item so that each has mean=0, and var of 1(z-standardization), Beta and Gamma are also learnt to minimize the data flactuation as having values between - and 1 might be too restrictive.\n",
    "\n",
    "new xj = (xj -meanj) / math.sqrt(var\\*\\*2 + epsilon)\n",
    "\n",
    "simplified version: `x = α * (x - μ) / (σ + ε) + β`\n",
    "\n",
    "gamma(multiplication) and beta(addition) will be learnt after this. epsilon is for numericalsatability as if denominator gets very small, overall number would be difficult to manage percision wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2789,  1.9644, -1.0588,  ..., -0.2789, -0.2197, -0.5883],\n",
       "         [-0.2754, -0.2754,  0.8985,  ...,  0.6412, -0.2754,  1.0653],\n",
       "         [-0.3145, -1.7347, -0.3145,  ..., -0.3145, -0.3714,  1.5210],\n",
       "         [-0.2987, -2.0208, -1.5840,  ..., -0.2987, -0.1121, -0.2987],\n",
       "         [-0.2669, -1.6686, -0.2669,  ..., -0.2669, -0.2669, -0.2669]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # gamma  # mulltiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # print(\"mean shape\", mean.shape, mean)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "ln = LayerNormalization()\n",
    "\n",
    "# print(\"Before normalization:\")\n",
    "# print(positional_encoded)\n",
    "\n",
    "normalized = ln(positional_encoded)\n",
    "# print(\"After normalization:\")\n",
    "print(normalized.shape)\n",
    "normalized  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. feed forward block\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "`FFN(x) = max(0, xW1 + b1)W2 + b2 (2)` # two lyers with ReLu in between\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feedforwardblock:\n",
      "torch.Size([1, 5, 512]) tensor([[[-0.2789,  1.9644, -1.0588,  ..., -0.2789, -0.2197, -0.5883],\n",
      "         [-0.2754, -0.2754,  0.8985,  ...,  0.6412, -0.2754,  1.0653],\n",
      "         [-0.3145, -1.7347, -0.3145,  ..., -0.3145, -0.3714,  1.5210],\n",
      "         [-0.2987, -2.0208, -1.5840,  ..., -0.2987, -0.1121, -0.2987],\n",
      "         [-0.2669, -1.6686, -0.2669,  ..., -0.2669, -0.2669, -0.2669]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7163,  0.3545,  0.2998,  ...,  0.0220,  0.0284, -0.0386],\n",
       "         [-0.1627,  0.2285,  0.4573,  ...,  0.5635,  0.0984, -0.2593],\n",
       "         [-0.2953, -0.0571, -0.1370,  ...,  0.3944,  0.3102, -0.4622],\n",
       "         [-0.4061, -0.3677, -0.0809,  ..., -0.1047, -0.0123, -0.4508],\n",
       "         [ 0.1250, -0.5352,  0.5249,  ...,  0.8480, -0.0298, -0.1069]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)  # first layer: w1,b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)  # second layer: w2,b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input:(batch, seq_len, d_model)\n",
    "\n",
    "        # after first layer: (batch, seq_len, d_ff)\n",
    "\n",
    "        # after second layer: (batch, seq_len, d_model)\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "feedforwardblock = FeedForwardBlock(d_model=512, d_ff=2048, dropout=0.5)\n",
    "\n",
    "print(\"Before feedforwardblock:\")\n",
    "print(normalized.shape, normalized)\n",
    "\n",
    "feedforwarded = feedforwardblock(normalized)\n",
    "# print(\"After normalization:\")\n",
    "print(feedforwarded.shape)\n",
    "feedforwarded  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-head attention\n",
    "\n",
    "takes input:(seq_len, d_model) of encoder and uses it three times k:key, q:query, v:values. then we multiply these matrices with Wk, Wq and Wv respectively. resulting in K',Q',V' of same(seq_len, d_model) dim. Now,split each of K', Q' and V' into h parts along d_model(embedding) dim where h is number of head. So that each head will have access to full sentence but different part of embedding of each token.\n",
    "\n",
    "Now, apply following formulas to each head which will result into h matrices of `(seq_len, d_k)` dims where `d_k` = `d_model/h`\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW^Q_i, K W^K_i, V W^V_i)\n",
    "$$\n",
    "\n",
    "Now concatenate all heads,\n",
    "\n",
    "$$\n",
    "\\text{MultiHead(Q, K, V)} = \\text{Concatenate}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^o\n",
    "$$\n",
    "\n",
    "![alt text](02_transformer/MHA.png)\n",
    "\n",
    "W^o is of `(seq, h*d_v)` shape where `d_v = d_k`\n",
    "\n",
    "and resultant MH-A is `(seq_len, d_model)` same as input\n",
    "\n",
    "But we also have to consider batch_dim for dealing with multiple sentences; the above intition works for single sentence.\n",
    "\n",
    "`SO WE WILL CONSIDER BATCH DIMENSION.`\n",
    "\n",
    "---\n",
    "\n",
    "### MASK\n",
    "\n",
    "before applying multiplying with V meaning\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "we get a scaled dot profuct of q and k, its (seq_len, seq_len) matrix. this shows interaction of each words with each other word.\n",
    "\n",
    "If we dont want some words to interact with other words, we basically replace there attention score(before applying softmax) with very small value, which means after softmax these values will become zero,so basically we hide attention between those two words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = dropout\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        # define weight matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # wq\n",
    "        self.wk = nn.Linear(d_model, d_model)  # wk\n",
    "        self.wv = nn.Linear(d_model, d_model)  # wv\n",
    "\n",
    "        # output matrix Wo (h*dv, d_model) where dv = dk\n",
    "        self.wo = nn.Linear(d_model, d_model)  # wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod  # so we could cal fn wothout specifying class instance\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]  # last dim of query/key/value\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  # transpose(-2, -1): transpose last two dims\n",
    "\n",
    "        # apply mask: just replace values you want to mask with very small values\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9)  # replace all values where mask==0 (conidtion is true) with -1e9\n",
    "\n",
    "        # applying softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len,seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        query = self.wq(q)\n",
    "        key = self.wq(k)\n",
    "        value = self.wq(v)\n",
    "\n",
    "        # splitting\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        # we moved h dimension because we want each head to consider (seq_len, d_k)\n",
    "        # each head considers full sentence but smaller embedding\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch, h, seq_len, d_k)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query=query, key=key, value=value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual/skip connection\n",
    "\n",
    "between add & Norm and previous layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):  # sublayer:previous layer\n",
    "        \"\"\"take x and combine with output of next layer\"\"\"\n",
    "\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoderblock\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n",
    "\n",
    "it will contain one multi-head attention, two Add&Norm, one Feed forward block and two residual connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        src_mask: mask we want to apply to input of encoder. we need this to hide interaction of padding word with other words.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # multihead attention within\n",
    "\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        The lambda is used because self_attention_block needs four arguments (query, key, value, mask) \n",
    "        but ResidualConnection expects a function that takes only one argument.\n",
    "        The lambda allows us to create a function that takes one argument x and expands it to the required four arguments, including the src_mask.\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "is made up of many encoder\n",
    "\n",
    "Each encoder block is repeated Nx times\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "        self.norm = LayerNormalization()  # at end\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # apply on layer after another # order matters\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 15-50-17.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output embeddings\n",
    "\n",
    "output embeddings are same as input embeddings, so weill just intialize it twice\n",
    "\n",
    "masked attention is some what same as self attenntion because of 3 same inputs while Mulihead attention block is actually cross attension bea=cause key and value are cping from encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        # we have three residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(3)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Args:\n",
    "            x: input of decoder\n",
    "            src_mask: mask applied to encoder\n",
    "            tgt_mask: target mask applied to decoder\n",
    "\n",
    "\n",
    "\n",
    "        src_mask and tgt_mask because we are dealing with language transalation. SO, source language is English and target language is italian\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # i. masked multihead attention: first residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))  # tgt_mask:becasue its decoder\n",
    "\n",
    "        # ii. cross attention: second residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        x = self.residual_connections[2](x, feedforwardblock)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"build decoder which is n times DecoderBlock one after anotherjust we did for encoder\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): whic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            # each layer is a decoderblock\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection/linear layer\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-23 10-50-34.png>)\n",
    "\n",
    "output of multihead attention is (seq_len, d_model)\n",
    "\n",
    "However we want to these words back into vocabularly which convert embedding to position in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_moel: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        this is a linear layer that is converting from d_model to vocab_size\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch seq_len, vocab_size)\n",
    "        # The purpose of applying softmax is to convert the raw output of the linear layer into a probability distribution over the vocabulary.\n",
    "        #  we will also apply softmax, specifically log_softmax for mathematiacal stability\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        \"\"\"\n",
    "        we need source embedding and target embedding because we are dealing with multiple languages\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # three methods, one to encoder, one to decode and one to project\n",
    "    # Not creating single forward method because we can reuse output of encoder and to also visualize the attention\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            src (_type_): src of language\n",
    "            src_mask (_type_): source mask\n",
    "        \"\"\"\n",
    "\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        src = self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_ouput, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_ouput, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    \"\"\"sumary_line\n",
    "\n",
    "    we need vocab size of src and tgt so get info about how many vectors to be created\n",
    "    Keyword arguments:\n",
    "\n",
    "    N: number of input layers i.e. number of enccoder blocks and number of decoder blocks\n",
    "    h: # of heads\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"strcuture will be same across all tasks\"\"\"\n",
    "\n",
    "    # create embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # positional encoding layers\n",
    "    # one encoding layer wold be enough\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "    # create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = feedforwardblock(d_model, d_ff, dropout)\n",
    "\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    # create encoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = feedforwardblock(d_model, d_ff, dropout)\n",
    "\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # now create encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # create projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # create transfromer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # initilize parameter to make trainig faster so they dont just strat with random values\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# engIt - task\n",
    "\n",
    "1. Download dataset: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it\n",
    "2. build tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer: create sentence into tokens. h=there are manyh tokenizers like BPE, subword-level,, word level, etc\n",
    "# we'll be creting word-level tokenizer i.e. split by space\n",
    "# so tokenizer builds vocab and maps tokens to index\n",
    "# there would be special tokens too for paddings, start of sentence, end of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train,py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohaib/anaconda3/envs/noManEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "# class that will train tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace  # to split words according to whitespace\n",
    "\n",
    "from pathlib import Path  # to assist in creating absolutes path using relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    # pasrsing each item which is a pair in dataset # (english, italian)\n",
    "    for item in ds:\n",
    "        yield item[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buils the tokeizer\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    \"\"\"\n",
    "    Building the tokenizer\n",
    "\n",
    "    Keyword arguments:\n",
    "    config -- config of our model\n",
    "    ds -- dataset\n",
    "    lang -- lang to build tokeizer for\n",
    "    \"\"\"\n",
    "\n",
    "    # file to save this tokenizer\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))  # mean we can change\n",
    "    #'../tokenizers/tokenizer_{0}.format(lang).json' same as f\"'../tokenizers/tokenizer_{lang}'\n",
    "\n",
    "    # so if tokenizer ddoesn't exists we create it\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()  # split by whitespces\n",
    "        #  now training tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)  # for a word to appear in vocab it must have min frequency of 2\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        print(tokenizer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and get tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "    # build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # keep 10% for val, 90 for trainig ... hf dst has single split so we'll be splitting manually\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # saving particular tokens to create the tensor for the model.so we need SOS, EOS, and PAD tokens so how are we going to assign ID to these tokens?\n",
    "        self.sos_token = torch.Tensor([tokenizer_src.token_to_id([\"[SOS]\"])], dtype=torch.int64)\n",
    "        self.eos_token = torch.Tensor([tokenizer_src.token_to_id([\"[EOS]\"])], dtype=torch.int64)\n",
    "        self.pad_token = torch.Tensor([tokenizer_src.token_to_id([\"[PAD]\"])], dtype=torch.int64)\n",
    "\n",
    "    # length of this dataset\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        # extracting original pair from hfDataset\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # converting text into IDs\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        #  adding padding to reach `seq_length` because model works with fixed length\n",
    "        # adding padding tokens\n",
    "\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # -2 for sos and eos token\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1  # -1 for sos and eos token\n",
    "        \"\"\"\n",
    "        while training, we only add SOS token on decoder side and on label side we add EOS token.\n",
    "        so for dec_num_padding_tokens we only need to add one of special tokens\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right to focus on this part. It's a crucial detail in sequence-to-sequence models like those used for machine translation. Let's break this down further:\n",
    "\n",
    "1. Encoder (Source Language) Side:\n",
    "\n",
    "   - We add both SOS (Start of Sequence) and EOS (End of Sequence) tokens.\n",
    "   - SOS tells the model \"The sentence is starting here.\"\n",
    "   - EOS tells the model \"The sentence ends here.\"\n",
    "   - This helps the encoder to understand the full context of the input sentence.\n",
    "\n",
    "2. Decoder (Target Language) Side:\n",
    "   - During training, we only add the SOS token at the beginning.\n",
    "   - The EOS token is not added to the input, but is expected in the output.\n",
    "\n",
    "Why this difference?\n",
    "\n",
    "1. For the Encoder:\n",
    "\n",
    "   - The full context is important. The model needs to know where the sentence starts and ends to encode all the information correctly.\n",
    "\n",
    "2. For the Decoder:\n",
    "   - During training, the decoder is typically fed the correct translation one word at a time (a technique called teacher forcing).\n",
    "   - It starts with SOS to know when to begin generating the translation.\n",
    "   - It should learn to generate EOS when it thinks the translation is complete.\n",
    "   - By not providing EOS in the input but expecting it in the output, we're teaching the model to decide when to stop generating.\n",
    "\n",
    "Practical example:\n",
    "\n",
    "Let's say we're translating \"Hello, how are you?\" from English to French.\n",
    "\n",
    "Encoder input might look like:\n",
    "[SOS] Hello , how are you ? [EOS]\n",
    "\n",
    "Decoder input during training might look like:\n",
    "[SOS] Bonjour , comment allez - vous ?\n",
    "\n",
    "And the expected output (label) would be:\n",
    "Bonjour , comment allez - vous ? [EOS]\n",
    "\n",
    "This way, the model learns to:\n",
    "\n",
    "1. Understand complete sentences (encoder)\n",
    "2. Start generating translations (decoder input)\n",
    "3. Know when to stop generating (decoder output)\n",
    "\n",
    "This subtle difference is key to training a model that can both understand full sentences and generate complete translations of varying lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2TokenizerFast' object has no attribute 'token_to_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cls_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_to_id\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m sep_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cls_token_id, sep_token_id)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2TokenizerFast' object has no attribute 'token_to_id'"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noManEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
