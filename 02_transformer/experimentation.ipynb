{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 512]),\n",
       " tensor([[[-0.7521,  0.6727,  0.7883,  ...,  0.4614,  0.3725,  0.7242],\n",
       "          [ 0.2382,  0.7201, -0.0123,  ..., -2.2650,  1.5683, -1.5002],\n",
       "          [ 0.1547,  1.3250, -1.6600,  ...,  0.8833,  1.1661,  0.8756],\n",
       "          [-0.6573,  0.0062, -0.2856,  ...,  1.6304, -0.2121, -0.0408],\n",
       "          [-0.6573,  0.0062, -0.2856,  ...,  1.6304, -0.2121, -0.0408]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 858,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we;ll be building input embeddings\n",
    "# allows to convert token into embedding of dim 1x52  : token -> input ID(position in vocab) ->embedding\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            d_model (int): dim of vector\n",
    "            vocab_size (int): # of words in vocab\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "# Create an example input tensor (batch size 1, sequence length 5, embedding dimension 20)\n",
    "batch_of_sentences = torch.tensor([[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "print(batch_of_sentences.shape)\n",
    "\n",
    "\n",
    "# Pass through the embedding layer\n",
    "# The forward method is called automatically when you use the instance like a function.\n",
    "embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "embedded_sentences.shape, embedded_sentences  # (batch, seq_len, embedding dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 6)"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "d_model = 6\n",
    "nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "print(x.shape)\n",
    "input_embeddings(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch\n",
    "x = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "print(x.shape)\n",
    "input_embeddings(x).shape  #(batch_size, seq_len, d_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 5, 512])\n",
      "x.shape[1]: 5\n",
      "self.pe:  torch.Size([1, 15, 512])\n",
      "self.pe[:, : x.shape[1], :]:  torch.Size([1, 15, 512])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input  tensor([[[-0.7521,  0.6727,  0.7883,  ...,  0.4614,  0.3725,  0.7242],\n",
      "         [ 0.2382,  0.7201, -0.0123,  ..., -2.2650,  1.5683, -1.5002],\n",
      "         [ 0.1547,  1.3250, -1.6600,  ...,  0.8833,  1.1661,  0.8756],\n",
      "         [-0.6573,  0.0062, -0.2856,  ...,  1.6304, -0.2121, -0.0408],\n",
      "         [-0.6573,  0.0062, -0.2856,  ...,  1.6304, -0.2121, -0.0408]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "input shape torch.Size([1, 5, 512])\n",
      "positional_encoded shape torch.Size([1, 5, 512])\n",
      "tensor([[[-1.5043,  0.0000,  1.5765,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 2.1593,  2.5208,  1.6191,  ..., -0.0000,  0.0000, -0.0000],\n",
      "         [ 2.1280,  0.0000, -1.4472,  ...,  3.7665,  2.3326,  0.0000],\n",
      "         [-1.0323, -0.0000, -0.0811,  ...,  5.2608, -0.4236,  0.0000],\n",
      "         [-0.0000, -0.0000, -1.8856,  ...,  5.2608, -0.4234,  1.9184]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "                Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "        order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "        tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "        bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "        as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "        learned and fixed [9].\n",
    "        In this work, we use sine and cosine functions of different frequencies:\n",
    "            `PE(pos,2i) = sin(pos/(10000)**2i/dmodel)`\n",
    "            `PE(pos,2i+1) = cos(pos/(10000)**2i/dmodel)`\n",
    "        where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "        corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "        chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "        relative positions, since for any fixed offset k, P E(pos+k) can be represented as a linear function of\n",
    "        PE(pos).\n",
    "\n",
    "        Keyword arguments:\n",
    "        dropout -- to make model less overfit\n",
    "        seq_len -- Specifies the maximum length of sequence that the model can handle. This helps determine the scale and range of the positional encodings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encodeing shape: seq_len X d_model i.e. each token will be represented (1*d_model) vector\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        formula :`PE(pos,2i) = cos(pos/(10000)**2i/dmodel) for i=1,3,5, ...and `PE(pos,2i) = sin(pos/(10000)**2i/dmodel) for i=2,4,6, ...and `\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #  Create a model of shape (seq_len , d_model)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #  create a vector of shape(seq_len,1) to represent position of word in sequence\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len,1)  # pos in formula\n",
    "        # create denominator of formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # apply sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # apply cos to odd positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # now we need to add batch dimension to these sentences so we can apply it to whole sentences, so to all the batch of sentence, because weill have batch of sentences.\n",
    "        # adding batch dim\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # register this tensor in buffer of module  .. it is done for the tensor that you want to keep inside the module, not as a lerarned parameter but you want it to be saved when you save the file of the model\n",
    "        # you should register it as a buffer. this way the tensor would be saved in file along with state of model\n",
    "        self.register_buffer(\"pe\", pe)  # This is typically used to register a buffer that should not to be considered a model parameter.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Say you have a linear layer nn.Linear. You already have weight and bias parameters. But if you need a new parameter you use register_parameter() to register a new named parameter that is a tensor.\n",
    "        When you register a new parameter it will appear inside the module.parameters() iterator, but when you register a buffer it will not.\n",
    "        The difference:\n",
    "        Buffers are named tensors that do not update gradients at every step, like parameters. For buffers, you create your custom logic (fully up to you).\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we need to add positional encoding to every token/word inside sequence/sentence\n",
    "        \"\"\"\n",
    "        print(\"x.shape:\",x.shape)\n",
    "        print(\"x.shape[1]:\",x.shape[1])\n",
    "        print(\"self.pe: \", self.pe.shape)\n",
    "        print(\"self.pe[:, : x.shape[1], :]: \", self.pe[:, :, :].shape)\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        \n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad_(False)  # x:token and pe is positional encoding  # because we dont want to learn pe because these are fixed\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "positional = PositionalEncoding(d_model=512, seq_len=15, dropout=0.5)\n",
    "\n",
    "# Create an example input tensor (batch size , sequence length , embedding dimension )\n",
    "\n",
    "# Apply positional encoding\n",
    "positional_encoded = positional(embedded_sentences)\n",
    "print(\"input \", embedded_sentences)\n",
    "\n",
    "print(\"input shape\", embedded_sentences.shape)\n",
    "print(\"positional_encoded shape\", positional_encoded.shape)\n",
    "\n",
    "print(positional_encoded)  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]),\n",
       " tensor([[-0.2874, -0.8507, -0.6007, -0.5157,  0.3350,  1.0351,  1.4875, -0.3185,\n",
       "          -2.7246,  1.7447]]))"
      ]
     },
     "execution_count": 863,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor\n",
    "tensorx = torch.randn(10,10)\n",
    "tensory = torch.randn(1, 10)\n",
    "tensorx.shape, tensory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]),\n",
       " tensor([[-0.5988,  0.7209,  0.9964, -0.4112, -0.1477, -1.2911,  0.9932, -0.1335,\n",
       "          -0.6351,  0.3323],\n",
       "         [-0.8539,  1.2648,  0.1975, -0.5337,  2.0465,  0.7591,  0.0898, -0.6929,\n",
       "          -0.2612,  0.7542],\n",
       "         [-0.8129,  2.2606,  0.2895, -0.9672,  0.2092,  1.4318,  0.4218, -0.0687,\n",
       "           0.7508,  1.6987],\n",
       "         [-0.3953, -0.8902, -1.4158,  0.7205,  1.3755, -0.6518, -0.0923,  0.2495,\n",
       "          -0.3197,  0.0689],\n",
       "         [-0.1325, -0.1278,  0.7780, -0.8491, -0.2670, -0.5789, -0.0672, -0.3224,\n",
       "          -1.9802, -1.6636]]))"
      ]
     },
     "execution_count": 864,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorx[::2,:].shape, tensorx[::2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]),\n",
       " tensor([[-0.5988, -1.2911],\n",
       "         [-0.8539,  0.7591],\n",
       "         [-0.8129,  1.4318],\n",
       "         [-0.3953, -0.6518],\n",
       "         [-0.1325, -0.5789]]))"
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorx[::2,::5].shape, tensorx[::2,::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 register_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule()\n",
      "Buffer tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Another buffer tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        # Register a buffer tensor with zeros\n",
    "        self.register_buffer(\"buffer_tensor\", torch.zeros(3, 3))\n",
    "\n",
    "        # Register another buffer tensor with specific values\n",
    "        data = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "        self.register_buffer(\"another_buffer\", data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the buffer tensors in the forward pass\n",
    "        output = x + self.buffer_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of MyModule\n",
    "model = MyModule()\n",
    "\n",
    "# Print the module to see its structure\n",
    "print(model)\n",
    "\n",
    "# Accessing the buffer tensors\n",
    "print(\"Buffer tensor:\")\n",
    "print(model.buffer_tensor)\n",
    "\n",
    "print(\"\\nAnother buffer tensor:\")\n",
    "print(model.another_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffers in PyTorch are a key feature that allows you to store tensors that are not parameters of a model but are still important for its operation. Here's an in-depth look at buffers, including their purpose, usage, and how they differ from parameters.\n",
    "\n",
    "## What are Buffers?\n",
    "\n",
    "Buffers are named tensors that are registered within a PyTorch `nn.Module`. Unlike model parameters, which are learned during training (e.g., weights and biases), buffers are typically used to store state information that should be preserved across model saves and loads, but does not require gradients.\n",
    "\n",
    "### Key Characteristics of Buffers\n",
    "\n",
    "1. **Non-learnable**: Buffers do not get updated during the training process through backpropagation. They are not optimized by any learning algorithm.\n",
    "2. **State Preservation**: Buffers are saved in the model's `state_dict`, allowing them to be restored when the model is loaded. This is useful for maintaining certain statistics or configurations that are necessary for the model's operation.\n",
    "\n",
    "3. **Device Management**: Buffers are automatically moved to the appropriate device (CPU or GPU) when the model is transferred between devices using `.to()`, ensuring that they are always in the correct context for computations.\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "Buffers are often used in scenarios where you need to maintain certain statistics or configurations that evolve over time but are not directly learned parameters. A common example is in batch normalization, where the running mean and variance are stored as buffers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update the running mean\n",
    "        self.mean = self.momentum * self.mean + (1 - self.momentum) * x.mean(dim=0)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `mean` is a buffer that keeps track of the running average of the input features.\n",
    "\n",
    "### Registering Buffers\n",
    "\n",
    "You can register a buffer in a PyTorch module using the `register_buffer()` method. This method takes the name of the buffer and the tensor you want to register.\n",
    "\n",
    "```python\n",
    "self.register_buffer('buffer_name', tensor)\n",
    "```\n",
    "\n",
    "### Differences Between Buffers and Parameters\n",
    "\n",
    "1. **Gradient Tracking**: Parameters are part of the model's learnable parameters and have gradients computed for them during training. Buffers do not have gradients and are not updated through optimization algorithms.\n",
    "\n",
    "2. **Inclusion in `parameters()`**: When you call the `parameters()` method on a module, it returns only the parameters. Buffers will not be included in this list.\n",
    "\n",
    "3. **Use Cases**: Buffers are typically used for maintaining state (like running statistics), while parameters are used for weights and biases that need to be learned from data.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- **Memory Management**: Buffers can help manage memory by storing intermediate computations or states that are needed later without the overhead of learning them.\n",
    "- **Checkpointing**: Buffers can be used in techniques like checkpointing, where you save the state of certain variables without saving the entire model, allowing for more efficient training and inference.\n",
    "\n",
    "- **Integration with Other Features**: Buffers can be used alongside other PyTorch features like hooks, which allow for custom operations during the forward and backward passes of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters automatically get moved to gpu with .to(device) but tensor dont that why we have to register them as buffer\n",
    "\n",
    "Yes, you are correct. When you move a PyTorch model to a device (e.g., a GPU) using .to(device), the model’s parameters are automatically moved to the specified device. However, regular tensors defined within the model are not automatically moved. To ensure that these tensors are moved along with the parameters, you register them as buffers.\n",
    "\n",
    "### Why Use Buffers?\n",
    "\n",
    "1. Automatic Device Management: Buffers are automatically moved to the specified device when you call .to(device) on the model.\n",
    "2. Consistency: Buffers maintain consistency in terms of device allocation, ensuring all parts of the model (parameters and non-parameters) are on the same device.\n",
    "3. Persistence: Buffers are saved and loaded along with the model’s parameters, making it easier to manage the model state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter device: cpu\n",
      "Regular tensor device (not moved): cpu\n",
      "Buffer device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.param = nn.Parameter(torch.randn(3, 3))  # A learnable parameter\n",
    "        self.buffer = torch.randn(3, 3)  # A regular tensor\n",
    "        self.register_buffer('registered_buffer', torch.randn(3, 3))  # A registered buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.param + self.registered_buffer\n",
    "\n",
    "# Create an instance of the module\n",
    "model = MyModule()\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Check device of model components\n",
    "print(\"Parameter device:\", model.param.device)\n",
    "print(\"Regular tensor device (not moved):\", model.buffer.device)\n",
    "print(\"Buffer device:\", model.registered_buffer.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add & Norm - layer normalization\n",
    "\n",
    "for each item in the batch, calculte mean & var, and normalize each item so that each has mean=0, and var of 1(z-standardization), Beta and Gamma are also learnt to minimize the data flactuation as having values between - and 1 might be too restrictive.\n",
    "\n",
    "new xj = (xj -meanj) / math.sqrt(var\\*\\*2 + epsilon)\n",
    "\n",
    "simplified version: `x = α * (x - μ) / (σ + ε) + β`\n",
    "\n",
    "gamma(multiplication) and beta(addition) will be learnt after this. epsilon is for numericalsatability as if denominator gets very small, overall number would be difficult to manage percision wise.\n",
    "\n",
    "![alt text](<Screenshot from 2024-07-22 14-45-21.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2170, -0.2799,  0.7022,  ..., -0.2799, -0.2799, -0.2799],\n",
       "         [ 1.0176,  1.2587,  0.6572,  ..., -0.4228, -0.4228, -0.4228],\n",
       "         [ 1.0056, -0.3246, -1.2293,  ...,  2.0299,  1.1335, -0.3246],\n",
       "         [-0.9406, -0.3104, -0.3599,  ...,  2.9010, -0.5690, -0.3104],\n",
       "         [-0.2604, -0.2604, -1.4239,  ...,  2.9857, -0.5217,  0.9233]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 869,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # gamma  # mulltiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # print(\"mean shape\", mean.shape, mean)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "ln = LayerNormalization()\n",
    "\n",
    "# print(\"Before normalization:\")\n",
    "# print(positional_encoded)\n",
    "\n",
    "normalized = ln(positional_encoded)\n",
    "# print(\"After normalization:\")\n",
    "print(normalized.shape)\n",
    "normalized  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. feed forward block\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "`FFN(x) = max(0, xW1 + b1)W2 + b2 (2)` # two lyers with ReLu in between\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feedforwardblock:\n",
      "torch.Size([1, 5, 512]) tensor([[[-1.2170, -0.2799,  0.7022,  ..., -0.2799, -0.2799, -0.2799],\n",
      "         [ 1.0176,  1.2587,  0.6572,  ..., -0.4228, -0.4228, -0.4228],\n",
      "         [ 1.0056, -0.3246, -1.2293,  ...,  2.0299,  1.1335, -0.3246],\n",
      "         [-0.9406, -0.3104, -0.3599,  ...,  2.9010, -0.5690, -0.3104],\n",
      "         [-0.2604, -0.2604, -1.4239,  ...,  2.9857, -0.5217,  0.9233]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0674, -0.1074, -0.2726,  ...,  0.5297,  0.0338,  0.2055],\n",
       "         [ 0.1020,  0.0711, -0.2530,  ...,  0.3117,  0.4551, -0.1430],\n",
       "         [ 0.0120,  0.6650,  0.6040,  ...,  0.4192,  0.4496,  0.1132],\n",
       "         [ 0.3258,  0.3699,  0.0034,  ...,  0.2887,  0.1953, -0.0397],\n",
       "         [ 0.3807, -0.1868, -0.1668,  ...,  1.1326,  0.4202, -0.3650]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)  # first layer: w1,b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)  # second layer: w2,b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input:(batch, seq_len, d_model)\n",
    "\n",
    "        # after first layer: (batch, seq_len, d_ff)\n",
    "\n",
    "        # after second layer: (batch, seq_len, d_model)\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "feedforwardblock = FeedForwardBlock(d_model=512, d_ff=2048, dropout=0.5)\n",
    "\n",
    "print(\"Before feedforwardblock:\")\n",
    "print(normalized.shape, normalized)\n",
    "\n",
    "feedforwarded = feedforwardblock(normalized)\n",
    "# print(\"After normalization:\")\n",
    "print(feedforwarded.shape)\n",
    "feedforwarded  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feedforwardblock:\n",
      "torch.Size([1, 5, 512]) tensor([[[-1.2170, -0.2799,  0.7022,  ..., -0.2799, -0.2799, -0.2799],\n",
      "         [ 1.0176,  1.2587,  0.6572,  ..., -0.4228, -0.4228, -0.4228],\n",
      "         [ 1.0056, -0.3246, -1.2293,  ...,  2.0299,  1.1335, -0.3246],\n",
      "         [-0.9406, -0.3104, -0.3599,  ...,  2.9010, -0.5690, -0.3104],\n",
      "         [-0.2604, -0.2604, -1.4239,  ...,  2.9857, -0.5217,  0.9233]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3161, -0.4402,  0.0345,  ...,  0.0255,  0.1650, -0.2611],\n",
       "         [-0.3515,  0.5928,  0.0686,  ...,  0.0961, -0.1577,  0.7342],\n",
       "         [ 0.1095,  0.0188, -0.1056,  ..., -0.2353, -0.2274,  0.1106],\n",
       "         [-0.2529,  0.0896,  0.3624,  ..., -0.1020,  0.0230,  0.6208],\n",
       "         [-0.3263,  0.2230,  0.6840,  ..., -0.2624,  0.2756, -0.1731]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int=512, d_ff: int=2048, dropout: float=0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)  # first layer: w1,b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)  # second layer: w2,b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input:(batch, seq_len, d_model)\n",
    "\n",
    "        # after first layer: (batch, seq_len, d_ff)\n",
    "\n",
    "        # after second layer: (batch, seq_len, d_model)\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "feedforwardblock = FeedForwardBlock()\n",
    "\n",
    "print(\"Before feedforwardblock:\")\n",
    "print(normalized.shape, normalized)\n",
    "\n",
    "feedforwarded = feedforwardblock(normalized)\n",
    "# print(\"After normalization:\")\n",
    "print(feedforwarded.shape)\n",
    "feedforwarded  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-head attention\n",
    "\n",
    "takes input:(seq_len, d_model) of encoder and uses it three times k:key, q:query, v:values. then we multiply these matrices with Wk, Wq and Wv respectively. resulting in K',Q',V' of same(seq_len, d_model) dim. Now,split each of K', Q' and V' into h parts along d_model(embedding) dim where h is number of head. So that each head will have access to full sentence but different part of embedding of each token.\n",
    "\n",
    "Now, apply following formulas to each head which will result into h matrices of `(seq_len, d_k)` dims where `d_k` = `d_model/h`\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW^Q_i, K W^K_i, V W^V_i)\n",
    "$$\n",
    "\n",
    "Now concatenate all heads,\n",
    "\n",
    "$$\n",
    "\\text{MultiHead(Q, K, V)} = \\text{Concatenate}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^o\n",
    "$$\n",
    "\n",
    "![alt text](MHA.png)\n",
    "\n",
    "W^o is of `(seq, h*d_v)` shape where `d_v = d_k`\n",
    "\n",
    "and resultant MH-A is `(seq_len, d_model)` same as input\n",
    "\n",
    "But we also have to consider batch_dim for dealing with multiple sentences; the above intition works for single sentence.\n",
    "\n",
    "`SO WE WILL CONSIDER BATCH DIMENSION.`\n",
    "\n",
    "---\n",
    "\n",
    "### MASK\n",
    "\n",
    "before applying multiplying with V meaning\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "we get a scaled dot profuct of q and k, its (seq_len, seq_len) matrix. this shows interaction of each words with each other word.\n",
    "\n",
    "If we dont want some words to interact with other words, we basically replace there attention score(before applying softmax) with very small value, which means after softmax these values will become zero,so basically we hide attention between those two words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        # define weight matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # wq\n",
    "        self.wk = nn.Linear(d_model, d_model)  # wk\n",
    "        self.wv = nn.Linear(d_model, d_model)  # wv\n",
    "\n",
    "        # output matrix Wo (h*dv, d_model) where dv = dk\n",
    "        self.wo = nn.Linear(d_model, d_model)  # wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod  # so we could cal fn wothout specifying class instance\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]  # last dim of query/key/value\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  # transpose(-2, -1): transpose last two dims\n",
    "\n",
    "        # apply mask: just replace values you want to mask with very small values\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9)  # replace all values where mask==0 (conidtion is true) with -1e9\n",
    "\n",
    "        # applying softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len,seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        query = self.wq(q)\n",
    "        print(\"q::::::::::::::::::::::::::::::::::::::::::::::::::::\", q.shape)\n",
    "        print(\"k::::::::::::::::::::::::::::::::::::::::::::::::::::\", k.shape)\n",
    "        print(\"v::::::::::::::::::::::::::::::::::::::::::::::::::::\", v.shape)\n",
    "        # ouput: q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
    "\n",
    "        key = self.wk(k)\n",
    "        value = self.wv(v)\n",
    "\n",
    "        # splitting\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        # we moved h dimension because we want each head to consider (seq_len, d_k)\n",
    "        # each head considers full sentence but smaller embedding\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch, h, seq_len, d_k)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query=query, key=key, value=value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        return self.wo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4,1 einsum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores:  torch.Size([8, 8, 350, 350])\n",
      "key:  torch.Size([8, 8, 350, 512])\n",
      "result:  torch.Size([8, 8, 350, 512])\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define query and key tensors with the specified shapes\n",
    "query = torch.randn(8,8, 350, 512)\n",
    "key = torch.randn(8,8, 350, 512)\n",
    "value = torch.randn(8,8, 350, 512)\n",
    "\n",
    "# Calculate using Einstein summation notation\n",
    "attention_scores = torch.einsum('bhij,bhkj->bhik', query, key)\n",
    "\n",
    "# print(\"Result shape:\", attention_scores.shape)  # Should print (8, 350, 350)\n",
    "# Calculate using Einstein summation notation\n",
    "# result = torch.einsum('bij,bjk->bik', query, key)\n",
    "\n",
    "\n",
    "# applying softmax\n",
    "attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len,seq_len)\n",
    "\n",
    "print(\"attention_scores: \",attention_scores.shape)\n",
    "print(\"key: \",key.shape)\n",
    "\n",
    "result = torch.einsum('bhsj,bhsk->bhsk', attention_scores, value)\n",
    "\n",
    "print(\"result: \",result.shape)\n",
    "\n",
    "print(\"=====================\")\n",
    "resultmat = attention_scores @ key\n",
    "# print(\"resultmat: \",resultmat[0][0][0][:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result (einsum):  tensor([-0.1943, -0.0143, -0.6272, -0.5324, -0.9999])\n",
      "=====================\n",
      "resultmat (matmul):  tensor([-0.3911, -0.9771,  0.0572, -0.1239, -1.1396])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Example dimensions\n",
    "batch_size = 2\n",
    "num_heads = 3\n",
    "seq_len = 4\n",
    "d_k = 5\n",
    "\n",
    "# Random tensors\n",
    "attention_scores = torch.randn(batch_size, num_heads, seq_len, seq_len)\n",
    "key = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "\n",
    "# Compute result using torch.einsum\n",
    "result = torch.einsum('bhsj,bhsk->bhsk', attention_scores, key)\n",
    "\n",
    "# Compute result using @ operator\n",
    "resultmat = attention_scores @ key\n",
    "\n",
    "# Print a sample of the results to compare\n",
    "print(\"result (einsum): \", result[0][0][0][:5])\n",
    "print(\"=====================\")\n",
    "print(\"resultmat (matmul): \", resultmat[0][0][0][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual/skip connection\n",
    "\n",
    "between add & Norm and previous layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):  # sublayer:previous layer\n",
    "        \"\"\"take x and combine with output of next layer\"\"\"\n",
    "\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoderblock\n",
    "\n",
    "![alt text](<Screenshot from 2024-07-22 14-45-21.png>)\n",
    "\n",
    "it will contain one multi-head attention, two Add&Norm, one Feed forward block and two residual connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        src_mask: mask we want to apply to input of encoder. we need this to hide interaction of padding word with other words.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # multihead attention within\n",
    "\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        The lambda is used because self_attention_block needs four arguments (query, key, value, mask) \n",
    "        but ResidualConnection expects a function that takes only one argument.\n",
    "        The lambda allows us to create a function that takes one argument x and expands it to the required four arguments, including the src_mask.\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "is made up of many encoder\n",
    "\n",
    "Each encoder block is repeated Nx times\n",
    "\n",
    "![alt text](<Screenshot from 2024-07-22 14-45-21.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "        self.norm = LayerNormalization()  # at end\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # apply on layer after another # order matters\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "![alt text](<Screenshot from 2024-07-22 15-50-17.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output embeddings\n",
    "\n",
    "output embeddings are same as input embeddings, so weill just intialize it twice\n",
    "\n",
    "masked attention is some what same as self attenntion because of 3 same inputs while Mulihead attention block is actually cross attension bea=cause key and value are cping from encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        # we have three residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(3)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Args:\n",
    "            x: input of decoder\n",
    "            src_mask: mask applied to encoder\n",
    "            tgt_mask: target mask applied to decoder\n",
    "\n",
    "\n",
    "\n",
    "        src_mask and tgt_mask because we are dealing with language transalation. SO, source language is English and target language is italian\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # i. masked multihead attention: first residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))  # tgt_mask:becasue its decoder\n",
    "\n",
    "        # ii. cross attention: second residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"build decoder which is n times DecoderBlock one after anotherjust we did for encoder\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): whic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            # each layer is a decoderblock\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection/linear layer\n",
    "\n",
    "![alt text](<Screenshot from 2024-07-23 10-50-34.png>)\n",
    "\n",
    "output of multihead attention is (seq_len, d_model)\n",
    "\n",
    "However we want to these words back into vocabularly which convert embedding to position in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        this is a linear layer that is converting from d_model to vocab_size\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch seq_len, vocab_size)\n",
    "        # The purpose of applying softmax is to convert the raw output of the linear layer into a probability distribution over the vocabulary.\n",
    "        #  we will also apply softmax, specifically log_softmax for mathematiacal stability\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        \"\"\"\n",
    "        we need source embedding and target embedding because we are dealing with multiple languages\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # three methods, one to encoder, one to decode and one to project\n",
    "    # Not creating single forward method because we can reuse output of encoder and to also visualize the attention\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            src (_type_): src of language\n",
    "            src_mask (_type_): source mask\n",
    "        \"\"\"\n",
    "\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        src = self.encoder(src, src_mask)\n",
    "        return src\n",
    "\n",
    "    def decode(self, encoder_ouput, src_mask, tgt, tgt_mask):\n",
    "        print(\"decodig.............................................................................\")\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_ouput, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    \"\"\"sumary_line\n",
    "\n",
    "    we need vocab size of src and tgt so get info about how many vectors to be created\n",
    "    Keyword arguments:\n",
    "\n",
    "    N: number of input layers i.e. number of enccoder blocks and number of decoder blocks\n",
    "    h: # of heads\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"strcuture will be same across all tasks\"\"\"\n",
    "\n",
    "    # create embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # positional encoding layers\n",
    "    # one encoding layer wold be enough\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "    # create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    # create encoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # now create encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # create projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # create transfromer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # initilize parameter to make trainig faster so they dont just strat with random values\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "# import numpy as np\n",
    "\n",
    "# # 1 input embeddings\n",
    "\n",
    "\n",
    "# class InputEmbeddings(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         d_model: int,\n",
    "#         vocab_size: int,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.vocab_size = vocab_size\n",
    "#         self.d_model = d_model\n",
    "#         self.embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"\\tsampleInputShape: (batch, seq_len) : \", x.shape)\n",
    "#         x = self.embedding(x)\n",
    "#         print(\"\\tinputEmbeddingsReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               x.shape)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "# # Create an example input tensor (batch size 1, sequence length 5, embedding dimension 20)\n",
    "# batch_of_sentences = torch.tensor(\n",
    "#     [[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "# # print(batch_of_sentences.shape)\n",
    "\n",
    "# # Pass through the embedding layer\n",
    "# # The forward method is called automatically when you use the instance like a function.\n",
    "# embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "# # print(\n",
    "# #     \"embedded_sentences.shape: \",\n",
    "# #     embedded_sentences.shape,\n",
    "# #     \" embedded_sentences: \",\n",
    "# #     embedded_sentences,\n",
    "# # )  # (batch, seq_len, embedding dim)\n",
    "\n",
    "# ######################################################################################################################\n",
    "\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model: int, seq_len: int, dropout: float):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.seq_len = seq_len\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         # build matrix of (seq_len, d_model)\n",
    "#         pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "#         #  create a vector of shape (seq_len,1) to represent position of word in sequence\n",
    "#         pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(\n",
    "#             1)  # (seq_len,1)  # pos in formula\n",
    "\n",
    "#         # create denominator\n",
    "#         div_term = torch.exp(\n",
    "#             torch.arange(0, d_model, 2).float() *\n",
    "#             (-math.log(10000.0) / d_model))\n",
    "\n",
    "#         # apply sin to even positions\n",
    "#         pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "\n",
    "#         # apply cos to odd positions\n",
    "#         pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "\n",
    "#         # print(\"pe.shape\", pe.shape)\n",
    "#         pe = pe.unsqueeze(0)  # (batch, seq_len, d_model)\n",
    "\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         we need to add positional encoding to every token/word inside sequence/sentence\n",
    "#         \"\"\"\n",
    "#         x = x + self.pe[:, :x.shape[1], :]  #\n",
    "#         # x:token and pe is positional encoding  # because we dont want to learn pe because these are fixed\n",
    "#         print(\"\\tpositionalEncodingReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               x.shape)\n",
    "\n",
    "#         #  :x.shape[1]:selecting just # of tokens because of input sequence length.\n",
    "#         return self.dropout(x)  # (batch, seq_len, d_model)\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# positional = PositionalEncoding(d_model=512, seq_len=35, dropout=0.5)\n",
    "# # print(\"positional:\", positional)\n",
    "\n",
    "# # Create an example input tensor (batch size , sequence length , embedding dimension )\n",
    "\n",
    "# # print(\"\\n\\n\\n input to PE\", embedded_sentences.shape)\n",
    "\n",
    "# positional_encoded = positional(embedded_sentences)\n",
    "# # print(\"input \", embedded_sentences)\n",
    "\n",
    "# # print(\"input shape\", embedded_sentences.shape)\n",
    "# # print(\"positional_encoded shape\", positional_encoded.shape)\n",
    "\n",
    "# # print(positional_encoded)  # (1, seq_len,d_model)\n",
    "\n",
    "# ######################################################################################################################\n",
    "\n",
    "\n",
    "# class LayerNormalization(nn.Module):\n",
    "#     def __init__(self, eps: float = 1e-6):\n",
    "#         super().__init__()\n",
    "#         self.eps = eps\n",
    "#         self.alpha = nn.Parameter(torch.ones(1))  # gamma  # mulltiplied\n",
    "#         self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(x.shape)\n",
    "#         mean = x.mean(dim=-1, keepdim=True)\n",
    "#         std = x.std(dim=-1, keepdim=True)\n",
    "#         # print(\"mean shape\", mean.shape, mean)\n",
    "#         normalized = self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "#         print(\"\\tnormalizedReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               normalized.shape)\n",
    "\n",
    "#         return normalized\n",
    "\n",
    "\n",
    "# ln = LayerNormalization()\n",
    "\n",
    "# # print(\"Before normalization:\")\n",
    "# # print(positional_encoded)\n",
    "\n",
    "# normalized = ln(positional_encoded)\n",
    "# # print(\"After normalization:\")\n",
    "# # print(normalized.shape)\n",
    "# # normalized  # (1, seq_len,d_model)\n",
    "\n",
    "# ##########################################################################################\n",
    "\n",
    "# # feed forward network\n",
    "\n",
    "\n",
    "# class FeedForwardBlock(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  d_model: int = 512,\n",
    "#                  d_ff: int = 2048,\n",
    "#                  dropout: float = 0.5):\n",
    "#         super().__init__()\n",
    "#         self.linear_1 = nn.Linear(d_model, d_ff, bias=True)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.linear_2 = nn.Linear(d_ff, d_model, bias=True)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # print(\"\\tbeforeFeedForwardReturnShape: (batch, seq_len, d_model) : \",\n",
    "#         #       x.shape)\n",
    "\n",
    "#         x = self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "#         print(\"\\tfeedForwardReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               x.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# feedforwardblock = FeedForwardBlock()\n",
    "\n",
    "# feedforwarded = feedforwardblock(normalized)\n",
    "\n",
    "# #############################################################################################\n",
    "\n",
    "# # Multi\n",
    "\n",
    "\n",
    "# class MultiHeadAttentionBlock(nn.Module):\n",
    "#     def __init__(self, d_model: int, h: int, dropout: float):\n",
    "#         super().__init__()\n",
    "#         self.d_model = d_model\n",
    "#         self.h = h\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "#         self.d_k = d_model // h\n",
    "#         attention_scores = None\n",
    "\n",
    "#         # weight matrices\n",
    "#         self.wq = nn.Linear(d_model, d_model)\n",
    "#         self.wk = nn.Linear(d_model, d_model)\n",
    "#         self.wv = nn.Linear(d_model, d_model)\n",
    "\n",
    "#         # output matrix :Wo (h*dv, d_model)\n",
    "#         self.wo = nn.Linear(d_model, d_model)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "#         d_k = query.shape[-1]  # extract embedding length\n",
    "\n",
    "#         # (b,h,seq_len,dk)->b,h,seq_len,seq_len)\n",
    "#         attention_scores = torch.einsum(\"bhij,bhkj->bhik\", query,\n",
    "#                                         key) / math.sqrt(d_k)\n",
    "\n",
    "#         if mask is not None:\n",
    "#             # replace all values with very small number so softwax will assign them 0 in output.\n",
    "#             attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "#         attention_scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "#         if dropout is not None:\n",
    "#             attention_scores = dropout(attention_scores)\n",
    "\n",
    "#         # (b,h,seq_len, seq_len) -> (b,h, seq_len, d_k)\n",
    "#         attention_scoresV = torch.einsum(\"bhsj, bhsk->bhsk\", attention_scores,\n",
    "#                                          value)\n",
    "\n",
    "#         print(\n",
    "#             \"\\t\\tmultiHeadAttentionReturnShape: (batch,heads, seq_len, d_k) : \",\n",
    "#             attention_scoresV.shape,\n",
    "#         )\n",
    "\n",
    "#         return (attention_scoresV, attention_scores)\n",
    "\n",
    "#     def forward(self, q, k, v, mask):\n",
    "#         query = self.wq(q)\n",
    "#         value = self.wv(v)\n",
    "#         key = self.wk(k)\n",
    "\n",
    "#         query = query.view(query.shape[0], query.shape[1], self.h, self.d_k)\n",
    "#         value = value.view(value.shape[0], value.shape[1], self.h, self.d_k)\n",
    "#         key = key.view(key.shape[0], key.shape[1], self.h, self.d_k)\n",
    "\n",
    "#         # swap seq_len and h so that we can consider each head as (seq_len, d_k)\n",
    "#         # (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "\n",
    "#         query = query.permute(0, 2, 1, 3)\n",
    "#         value = value.permute(0, 2, 1, 3)\n",
    "#         key = key.permute(0, 2, 1, 3)\n",
    "\n",
    "#         # (batch, h,seq_len, d_k)\n",
    "#         x, self.attention_scores = MultiHeadAttentionBlock.attention(\n",
    "#             query, key, value, mask, self.dropout)\n",
    "\n",
    "#         # reverting shape permutation: (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k)\n",
    "#         x = x.permute(0, 2, 1, 3)\n",
    "\n",
    "#         # (batch, seq_len, h, d_k) -->(batch, seq_len, d_model)\n",
    "#         x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "#         x = self.wo(x)\n",
    "\n",
    "#         print(\"\\tmhaReturnShape: (batch, seq_len, d_model) : \", x.shape)\n",
    "\n",
    "#         # (batch, seq_len, d_model\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # Create an instance of MultiHeadAttentionBlock\n",
    "# MHA = MultiHeadAttentionBlock(d_model=512, h=8, dropout=0.5)\n",
    "\n",
    "# # Create random input tensors\n",
    "# batch_size = 8\n",
    "# seq_len = 10\n",
    "# d_model = 512\n",
    "\n",
    "# # Create random tensors for query, key, and value\n",
    "# # Shape: (batch_size, seq_len, d_model)\n",
    "# q = torch.randn(batch_size, seq_len, d_model)\n",
    "# k = torch.randn(batch_size, seq_len, d_model)\n",
    "# v = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# # No mask for this example\n",
    "# mask = None\n",
    "\n",
    "# # Call the MultiHeadAttentionBlock\n",
    "# mha = MHA(q, k, v, mask)\n",
    "\n",
    "# ##################################################\n",
    "\n",
    "\n",
    "# class ResidualConnection(nn.Module):\n",
    "#     def __init__(self, dropout: float):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.norm = LayerNormalization()\n",
    "\n",
    "#     def forward(self, x, sublayer):\n",
    "#         # x is input\n",
    "#         return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "# ###################################################\n",
    "\n",
    "\n",
    "# class EncoderBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         self_attention_block: MultiHeadAttentionBlock,\n",
    "#         feed_forward_block: FeedForwardBlock,\n",
    "#         dropout: float,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.self_attention_block = self_attention_block  # bve\n",
    "#         self.feed_forward_block = feed_forward_block\n",
    "#         self.residual_connection = nn.ModuleList(\n",
    "#             [ResidualConnection(dropout) for _ in range(2)])\n",
    "\n",
    "#     def forward(self, x, src_mask):\n",
    "#         x = self.residual_connection[0](\n",
    "#             x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "#         x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "#         print(\"\\tencoderBlockReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               x.shape)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# ###################################################\n",
    "# # for N encoder blocks\n",
    "\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, layers: nn.ModuleList):\n",
    "#         super().__init__()\n",
    "#         self.layers = layers  # this would be encoderBlock\n",
    "#         self.norm = LayerNormalization()  # layerNorm at end\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         for i, layer in enumerate(self.layers):\n",
    "#             print(f\"\\n\\nEncoder Block:{i+1} \\n\")\n",
    "#             x = layer(x, mask)\n",
    "#         x = self.norm(x)\n",
    "#         print(\"\\nencoderReturnShape: (batch, seq_len, d_model) : \", x.shape)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# ######################################################3333\n",
    "# # # Example usage\n",
    "# # d_model = 512\n",
    "# # vocab_size = 1000\n",
    "# # seq_len = 35\n",
    "# # dropout = 0.5\n",
    "# # num_layers = 6\n",
    "# # num_heads = 8\n",
    "# # d_ff = 2048\n",
    "\n",
    "# # # Create instances\n",
    "# # input_embeddings = InputEmbeddings(d_model=d_model, vocab_size=vocab_size)\n",
    "# # positional_encoding = PositionalEncoding(d_model=d_model,\n",
    "# #                                          seq_len=seq_len,\n",
    "# #                                          dropout=dropout)\n",
    "# # feed_forward_block = FeedForwardBlock(d_model=d_model,\n",
    "# #                                       d_ff=d_ff,\n",
    "# #                                       dropout=dropout)\n",
    "# # self_attention_block = MultiHeadAttentionBlock(d_model=d_model,\n",
    "# #                                                h=num_heads,\n",
    "# #                                                dropout=dropout)\n",
    "\n",
    "# # encoder_blocks = nn.ModuleList([\n",
    "# #     EncoderBlock(self_attention_block, feed_forward_block, dropout)\n",
    "# #     for _ in range(num_layers)\n",
    "# # ])\n",
    "# # encoder = Encoder(layers=encoder_blocks)\n",
    "\n",
    "# # # Create example input\n",
    "# # batch_size = 1\n",
    "# # batch_of_sentences = torch.tensor(\n",
    "# #     [[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "# # embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "# # positional_encoded = positional_encoding(embedded_sentences)\n",
    "\n",
    "# # # Run through the encoder\n",
    "# # mask = None  # Assuming no mask for simplicity\n",
    "# # encoder_output = encoder(positional_encoded, mask)\n",
    "\n",
    "# # #################################################\n",
    "\n",
    "# # # output embeddings are same as input embeddings\n",
    "\n",
    "\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         self_attention_block: MultiHeadAttentionBlock,\n",
    "#         cross_attention_block: MultiHeadAttentionBlock,\n",
    "#         feed_forward_block: FeedForwardBlock,\n",
    "#         dropout: float,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.self_attention_block = self_attention_block\n",
    "#         self.cross_attention_block = cross_attention_block\n",
    "#         self.feed_forward_block = feed_forward_block\n",
    "#         self.residual_connections = nn.ModuleList(\n",
    "#             [ResidualConnection(dropout) for _ in range(3)])\n",
    "\n",
    "#     def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "#         x = self.residual_connections[0](\n",
    "#             x, lambda t: self.self_attention_block(t, t, t, tgt_mask))\n",
    "#         x = self.residual_connections[1](\n",
    "#             x,\n",
    "#             lambda t: self.cross_attention_block(t, encoder_output,\n",
    "#                                                  encoder_output, src_mask),\n",
    "#         )\n",
    "#         x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "#         print(\"\\tdecoderBlockReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               x.shape)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, layers: nn.ModuleList):\n",
    "#         super().__init__()\n",
    "#         self.layers = layers\n",
    "#         self.norm = LayerNormalization()\n",
    "\n",
    "#     def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "#         for i, layer in enumerate(self.layers):\n",
    "#             print(f\"\\n\\nDecoder Block:{i+1} \\n\")\n",
    "#             x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "#         x = self.norm(x)\n",
    "#         print(\"\\ndecoderReturnShape: (batch, seq_len, d_model) : \", x.shape)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class ProjectionLayer(nn.Module):\n",
    "#     def __init__(self, d_model: int, vocab_size: int):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.log_softmax(self.proj(x), dim=-1)\n",
    "#         print(\"\\nprojectionLayerReturnShape: (batch, seq_len, d_model) : \",\n",
    "#               x.shape)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# ####################################################\n",
    "\n",
    "# # d_model = 512\n",
    "# # vocab_size = 1000\n",
    "# # seq_len = 35\n",
    "# # dropout = 0.5\n",
    "# # num_layers = 6\n",
    "# # num_heads = 8\n",
    "# # d_ff = 2048\n",
    "\n",
    "# # input_embeddings = InputEmbeddings(d_model=d_model, vocab_size=vocab_size)\n",
    "# # positional_encoding = PositionalEncoding(d_model=d_model,\n",
    "# #                                          seq_len=seq_len,\n",
    "# #                                          dropout=dropout)\n",
    "# # feed_forward_block = FeedForwardBlock(d_model=d_model,\n",
    "# #                                       d_ff=d_ff,\n",
    "# #                                       dropout=dropout)\n",
    "# # self_attention_block = MultiHeadAttentionBlock(d_model=d_model,\n",
    "# #                                                h=num_heads,\n",
    "# #                                                dropout=dropout)\n",
    "\n",
    "# # projection_layer = ProjectionLayer(d_model, vocab_size)\n",
    "\n",
    "# # encoder_blocks = nn.ModuleList([\n",
    "# #     EncoderBlock(self_attention_block, feed_forward_block, dropout)\n",
    "# #     for _ in range(num_layers)\n",
    "# # ])\n",
    "# # encoder = Encoder(layers=encoder_blocks)\n",
    "\n",
    "# # # Decoder specific components\n",
    "# # cross_attention_block = MultiHeadAttentionBlock(d_model=d_model,\n",
    "# #                                                 h=num_heads,\n",
    "# #                                                 dropout=dropout)\n",
    "# # decoder_blocks = nn.ModuleList([\n",
    "# #     DecoderBlock(self_attention_block, cross_attention_block,\n",
    "# #                  feed_forward_block, dropout) for _ in range(num_layers)\n",
    "# # ])\n",
    "# # decoder = Decoder(layers=decoder_blocks)\n",
    "\n",
    "# # # Create example input\n",
    "# # batch_size = 1\n",
    "# # batch_of_sentences = torch.tensor(\n",
    "# #     [[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "# # embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "# # positional_encoded = positional_encoding(embedded_sentences)\n",
    "\n",
    "# # # Run through the encoder\n",
    "# # mask = None  # Assuming no mask for simplicity\n",
    "# # encoder_output = encoder(positional_encoded, mask)\n",
    "\n",
    "# # # Create example decoder input\n",
    "# # decoder_input = torch.tensor([[1, 2, 3, 4, 5]])  # Shape: (batch_size, seq_len)\n",
    "# # decoder_embedded = input_embeddings(decoder_input)\n",
    "# # decoder_positional_encoded = positional_encoding(decoder_embedded)\n",
    "\n",
    "# # # Run through the decoder\n",
    "# # tgt_mask = None  # Assuming no mask for simplicity\n",
    "# # decoder_output = decoder(decoder_positional_encoded, encoder_output, mask,\n",
    "# #                          tgt_mask)\n",
    "\n",
    "# # projecteded = projection_layer(decoder_output)\n",
    "\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# #############################o##############################\n",
    "# # Transformer\n",
    "\n",
    "\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         encoder: Encoder,\n",
    "#         decoder: Decoder,\n",
    "#         src_embed: InputEmbeddings,\n",
    "#         tgt_embed: InputEmbeddings,\n",
    "#         src_pos: PositionalEncoding,\n",
    "#         tgt_pos: PositionalEncoding,\n",
    "#         projection_layer: ProjectionLayer,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.src_embed = src_embed\n",
    "#         self.tgt_embed = tgt_embed\n",
    "#         self.src_pos = src_pos\n",
    "#         self.tgt_pos = tgt_pos\n",
    "#         self.projection_layer = projection_layer\n",
    "\n",
    "#     # three methods, one to encoder, one to decode and one to project\n",
    "#     # Not creating single forward method because we can reuse output of encoder and to also visualize the attention\n",
    "\n",
    "#     def encode(self, src, src_mask):\n",
    "#         src = self.src_embed(src)\n",
    "#         src = self.src_pos(src)\n",
    "#         src = self.encoder(src, mask)\n",
    "#         return src\n",
    "\n",
    "#     def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "#         tgt = self.tgt_embed(tgt)\n",
    "#         tgt = self.tgt_pos(tgt)\n",
    "#         tgt = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "#         return tgt\n",
    "\n",
    "#     def project(self, x):\n",
    "#         return self.projection_layer(x)\n",
    "\n",
    "\n",
    "# #####################################################################\n",
    "\n",
    "\n",
    "# def build_transformer(\n",
    "#     src_vocab_size: int,\n",
    "#     tgt_vocab_size: int,\n",
    "#     src_seq_len: int,\n",
    "#     tgt_seq_len: int,\n",
    "#     d_model: int,\n",
    "#     N:int=6,\n",
    "#     h:int=8,\n",
    "#     dropout: float = 0.5,\n",
    "#     d_ff:int=2048,\n",
    "# ):\n",
    "    \n",
    "#     \"\"\"\n",
    "#     we need vocab size of src and tgt so get info about how many vectors to be created\n",
    "#     Keyword arguments:\n",
    "\n",
    "#     N: number of input layers i.e. number of enccoder blocks and number of decoder blocks\n",
    "#     h: # of heads\n",
    "#     \"\"\"\n",
    "#     \"\"\"strcuture will be same across all tasks\"\"\"\n",
    "#     src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "#     tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "#     # positional encoding layers\n",
    "#     # one encoding layer wold be enough\n",
    "#     src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "#     tgt_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "#     # create encoder blocks\n",
    "#     encoder_blocks = []\n",
    "#     for _ in range(N):\n",
    "#         encoder_self_attention_block = MultiHeadAttentionBlock(\n",
    "#             d_model, h, dropout)\n",
    "#         feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "#         encoder_block = EncoderBlock(encoder_self_attention_block,\n",
    "#                                      feed_forward_block, dropout)\n",
    "#         encoder_blocks.append(encoder_block)\n",
    "\n",
    "#     # create encoder blocks\n",
    "#     decoder_blocks = []\n",
    "#     for _ in range(N):\n",
    "#         decoder_self_attention_block = MultiHeadAttentionBlock(\n",
    "#             d_model, h, dropout)\n",
    "#         decoder_cross_attention_block = MultiHeadAttentionBlock(\n",
    "#             d_model, h, dropout)\n",
    "#         feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "#         decoder_block = DecoderBlock(\n",
    "#             decoder_self_attention_block,\n",
    "#             decoder_cross_attention_block,\n",
    "#             feed_forward_block,\n",
    "#             dropout,\n",
    "#         )\n",
    "#         decoder_blocks.append(decoder_block)\n",
    "\n",
    "#     encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "#     decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "#     # projection layer\n",
    "#     projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "#     # create transfromer\n",
    "#     transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos,\n",
    "#                               tgt_pos, projection_layer)\n",
    "\n",
    "#     # initilize parameter to make trainig faster so they dont just strat with random values\n",
    "#     for p in transformer.parameters():\n",
    "#         if p.dim() > 1:\n",
    "#             nn.init.xavier_uniform_(p)\n",
    "#     return transformer\n",
    "\n",
    "\n",
    "# build_transformer(\n",
    "#     src_vocab_size=1000,\n",
    "#     tgt_vocab_size=1000,\n",
    "#     src_seq_len=100,\n",
    "#     tgt_seq_len=100,\n",
    "#     d_model=512,\n",
    "#     h=8,\n",
    "#     dropout=0.5,\n",
    "#     d_ff=2048,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# engIt - task\n",
    "\n",
    "1. Download dataset: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it\n",
    "2. build tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer: create sentence into tokens. h=there are manyh tokenizers like BPE, subword-level,, word level, etc\n",
    "# we'll be creting word-level tokenizer i.e. split by space\n",
    "# so tokenizer builds vocab and maps tokens to index\n",
    "# there would be special tokens too for paddings, start of sentence, end of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train,py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "# class that will train tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace  # to split words according to whitespace\n",
    "\n",
    "from pathlib import Path  # to assist in creating absolutes path using relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    # pasrsing each item which is a pair in dataset # (english, italian)\n",
    "    # print(\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nitem:\", item)\n",
    "    \n",
    "    for item in ds:\n",
    "        sentence = item[\"translation\"][lang]\n",
    "        print(f\"Sentence ({lang}): {sentence}\")\n",
    "        yield item[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buils the tokeizer\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    \"\"\"\n",
    "    Building the tokenizer\n",
    "\n",
    "    Keyword arguments:\n",
    "    config -- config of our model\n",
    "    ds -- dataset\n",
    "    lang -- lang to build tokeizer for\n",
    "    \"\"\"\n",
    "\n",
    "    # file to save this tokenizer\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))  # mean we can change\n",
    "    #'../tokenizers/tokenizer_{0}.format(lang).json' same as f\"'../tokenizers/tokenizer_{lang}'\n",
    "\n",
    "    # so if tokenizer ddoesn't exists we create it\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()  # split by whitespces\n",
    "        #  now training tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)  # for a word to appear in vocab it must have min frequency of 2\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# def get_or_build_tokenizer(config, ds, lang):\n",
    "#     \"\"\"lang: language to build tokenizer for\"\"\"\n",
    "#     # config['tokenizer_file'] = '../tokenizers/tokenizer_{0}'\n",
    "#     tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n",
    "#     if not Path.exists(tokenizer_path):\n",
    "#         tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "#         # split by wordspace\n",
    "#         tokenizer.pre_tokenizers = Whitespace()\n",
    "#         trainer = WordLevelTrainer(\n",
    "#             special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "#             min_frequency=2)\n",
    "\n",
    "#         print(\"tokenizer training started...\")\n",
    "#         tokenizer.train_from_iterator(get_all_sentences(ds, lang),\n",
    "#                                       trainer=trainer)\n",
    "#         tokenizer.save(str(tokenizer_path))\n",
    "#     else:\n",
    "#         tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "#     print(\"tokenizer initiated!\")\n",
    "#     return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and get tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n\\n\\nds_raw: {ds_raw}\")\n",
    "    # build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "    \n",
    "\n",
    "\n",
    "    # keep 10% for val, 90 for trainig ... hf dst has single split so we'll be splitting manually\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    #\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # max sequence length of src and tgt in both splits\n",
    "    max_len_src, max_len_tgt = 0, 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        # print(\"\\n\\n item:\", item)\n",
    "\n",
    "        # load each sentence , convert it to ids using tokenizer and i check length.\n",
    "        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n",
    "        # print(\"\\n\\nsrc_ids:\", src_ids )\n",
    "        # print(\"\\n\\tgt_ids:\", tgt_ids )\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        \n",
    "    print(f\"Max len src:{max_len_src}\")\n",
    "    print(f\"Max len tgt:{max_len_tgt}\")\n",
    "\n",
    "    print(f\"Max length of src sentence{max_len_src}\")\n",
    "    print(f\"Max length of tgt sentence{max_len_tgt}\")\n",
    "\n",
    "    # data loader\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)  # batch_size=1 because we want to process each sentence one by one\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import build_transformer\n",
    "\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        config (_type_): _description_\n",
    "        vocab_src_len (_type_): source vocab size\n",
    "        vocab_tgt_len (_type_): target vocab size\n",
    "    \"\"\"\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"], config[\"d_model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # saving particular tokens to create the tensor for the model.so we need SOS, EOS, and PAD tokens so how are we going to assign ID to these tokens?\n",
    "        self.sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    # length of this dataset\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # extracting original pair from hfDataset\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # converting text into IDs\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        #  adding padding to reach `seq_length` because model works with fixed length\n",
    "        # adding padding tokens\n",
    "\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # -2 for sos and eos token\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1  # -1 for sos token  # eos will be generated by model\n",
    "        \"\"\"\n",
    "        while training,on decode side, we only add SOS and on label side we add EOS token.\n",
    "        so for dec_num_padding_tokens we only need to add one of special tokens\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure our seq_len is enough to represent all of sentences in our dataset. i.e. padding must never become negative\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # lets build three tensors for encoder input and decoder input and also for lebel. So, one tensor would be send to encoder,\n",
    "        # one to decoder input and one that we expect as decoder's output and that output will be called label/target\n",
    "\n",
    "        # encoder input = sos + src_text + eos + pad_tokens\n",
    "        encoder_input = torch.cat([self.sos_token, torch.tensor(enc_input_tokens, dtype=torch.int64), self.eos_token, torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)])\n",
    "\n",
    "        # decoder input = sos + decoder input + padtokens\n",
    "        decoder_input = torch.cat([self.sos_token, torch.tensor(dec_input_tokens, dtype=torch.int64), torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)])\n",
    "\n",
    "        # label: decoder output that we expect=  decoder input +eos+ padtokens (add EOS to label)\n",
    "        label = torch.cat([torch.tensor(dec_input_tokens, dtype=torch.int64), self.eos_token, torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)])\n",
    "\n",
    "\n",
    "        print(f\"\\t\\t\\t\\tPadding added\")\n",
    "        print(f\"\\t\\t\\t\\tsequenceTextLimit: {self.seq_len}\")\n",
    "        print(f\"\\t\\t\\t\\tsrctextShape: {encoder_input.shape}\")\n",
    "        print(f\"\\t\\t\\t\\ttgttextShape: {decoder_input.shape}\")\n",
    "        print(f\"\\t\\t\\t\\tlabeltextShap: {label.shape}\")\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        \"\"\"\n",
    "        In encoder inout, we are adding padding to match length but we dont want these paddings to effect in attenation mechanism so we'll mask these paddings \n",
    "        that will indicate that these tokens shouldn't be considered: encoder_mask \n",
    "        it says all the tokens that are not padding are OK.        \n",
    "        -------\n",
    "        \n",
    "        for decoder we need a specail mask called causal mask which means each word can only look at previous token and non-padding token. \n",
    "        Remember we only want real words to be considered in self attention.\n",
    "        \n",
    "        encoder_input: (seq,_len)  \n",
    "        Decoder_input: (seq,_len)  \n",
    "        encoder_mask: (1, batch, seq,_len)  \n",
    "        decoder_mask: (1,batch, seq_len) & (1,seq_len, seq_len)\n",
    "        label:(seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        # dc_mask1singlesqueeze = (decoder_input != self.pad_token).unsqueeze(0).int()\n",
    "        # print(\"dc_mask1singlesqueeze: \",dc_mask1singlesqueeze.shape)\n",
    "        # dc_mask1douoblesqueeze = (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int()\n",
    "        # print(\"dc_mask1douoblesqueeze: \",dc_mask1douoblesqueeze.shape)\n",
    "        \n",
    "        # causal_mask1 = causal_mask(decoder_input.size(0))\n",
    "        # print(\"causal_mask1: \",causal_mask1.shape)\n",
    "        # #############3\n",
    "        \n",
    "        \n",
    "        # productsinglesqueeze = (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0))\n",
    "        # print(\"productsinglesqueeze: \",dc_mask1singlesqueeze.shape)\n",
    "        # productdoublesqueeze = (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0))\n",
    "        # print(\"productdoublesqueeze: \",productdoublesqueeze.shape)\n",
    "        \n",
    "        # dc_mask2 = (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0))\n",
    "        # print(\"dc_mask2: \",dc_mask2.shape)\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }  # (seq_len)  # (seq_len)  # (1, batch, seq,_len)  # causal_mask will build matrix of seq_len * seq_len     # (1,batch, seq_len) & (1,seq_len, seq_len)\n",
    "\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"\n",
    "    causal_mask will build matrix of seq_len * seq_len     # (1,batch, seq_len) & (1,seq_len, seq_len)\n",
    "    we want each word in decider to only watch non padding words that come before it.\n",
    "    Below matrix represent K\\*Q in softmax attention, we want to hide all the values above the diagonal.\n",
    "    so we want all values above diagonal to be masked out.\"\"\"\n",
    "\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    print(mask[:50,:50])\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right to focus on this part. It's a crucial detail in sequence-to-sequence models like those used for machine translation. Let's break this down further:\n",
    "\n",
    "1. Encoder (Source Language) Side:\n",
    "\n",
    "   - We add both SOS (Start of Sequence) and EOS (End of Sequence) tokens.\n",
    "   - SOS tells the model \"The sentence is starting here.\"\n",
    "   - EOS tells the model \"The sentence ends here.\"\n",
    "   - This helps the encoder to understand the full context of the input sentence.\n",
    "\n",
    "2. Decoder (Target Language) Side:\n",
    "   - During training, we only add the SOS token at the beginning.\n",
    "   - The EOS token is not added to the input, but is expected in the output.\n",
    "\n",
    "Why this difference?\n",
    "\n",
    "1. For the Encoder:\n",
    "\n",
    "   - The full context is important. The model needs to know where the sentence starts and ends to encode all the information correctly.\n",
    "\n",
    "2. For the Decoder:\n",
    "   - During training, the decoder is typically fed the correct translation one word at a time (a technique called teacher forcing).\n",
    "   - It starts with SOS to know when to begin generating the translation.\n",
    "   - It should learn to generate EOS when it thinks the translation is complete.\n",
    "   - By not providing EOS in the input but expecting it in the output, we're teaching the model to decide when to stop generating.\n",
    "\n",
    "Practical example:\n",
    "\n",
    "Let's say we're translating \"Hello, how are you?\" from English to French.\n",
    "\n",
    "Encoder input might look like:\n",
    "[SOS] Hello , how are you ? [EOS]\n",
    "\n",
    "Decoder input during training might look like:\n",
    "[SOS] Bonjour , comment allez - vous ?\n",
    "\n",
    "And the expected output (label) would be:\n",
    "Bonjour , comment allez - vous ? [EOS]\n",
    "\n",
    "This way, the model learns to:\n",
    "\n",
    "1. Understand complete sentences (encoder)\n",
    "2. Start generating translations (decoder input)\n",
    "3. Know when to stop generating (decoder output)\n",
    "\n",
    "This subtle difference is key to training a model that can both understand full sentences and generate complete translations of varying lengths.\n",
    "\n",
    "### causal Mask\n",
    "\n",
    "we want each word in decider to only watch non padding words that come before it.\n",
    "Below matrix represent K\\*Q in softmax attention, we want to hide all the values above the diagonal.\n",
    "so we want all values above diagonal to be masked out.\n",
    "![alt text](<Screenshot from 2024-07-29 10-43-45.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True])"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3]) != 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, 10, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": None,  # preload to restart training if crashed\n",
    "        \"tokenizer_file\": \"tokenizer{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\",\n",
    "    }\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config[\"model_folder\"]\n",
    "    model_basename = config[\"model_basename\"]\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path(\".\") / model_folder / model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "config: {'batch_size': 8, 'num_epochs': 20, 'lr': 0.0001, 'seq_len': 350, 'd_model': 512, 'lang_src': 'en', 'lang_tgt': 'it', 'model_folder': 'weights', 'model_basename': 'tmodel_', 'preload': None, 'tokenizer_file': 'tokenizer{0}.json', 'experiment_name': 'runs/tmodel'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ds_raw: Dataset({\n",
      "    features: ['id', 'translation'],\n",
      "    num_rows: 32332\n",
      "})\n",
      "Max len src:309\n",
      "Max len tgt:274\n",
      "Max length of src sentence309\n",
      "Max length of tgt sentence274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch: 00:   0%|          | 0/3638 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "batch: {'encoder_input': tensor([[   2, 1432,   86,  ...,    1,    1,    1],\n",
      "        [   2,   87,    9,  ...,    1,    1,    1],\n",
      "        [   2,   88,   12,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,  100,  345,  ...,    1,    1,    1],\n",
      "        [   2,  257,    9,  ...,    1,    1,    1],\n",
      "        [   2,   35,    9,  ...,    1,    1,    1]]), 'decoder_input': tensor([[    2,  3017,    13,  ...,     1,     1,     1],\n",
      "        [    2,    52,    44,  ...,     1,     1,     1],\n",
      "        [    2,   187, 11554,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    2,  1367,   857,  ...,     1,     1,     1],\n",
      "        [    2,   382,   272,  ...,     1,     1,     1],\n",
      "        [    2,     9,   389,  ...,     1,     1,     1]]), 'encoder_mask': tensor([[[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'decoder_mask': tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'label': tensor([[ 3017,    13,    59,  ...,     1,     1,     1],\n",
      "        [   52,    44,   586,  ...,     1,     1,     1],\n",
      "        [  187, 11554,  4430,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [ 1367,   857,    39,  ...,     1,     1,     1],\n",
      "        [  382,   272,     5,  ...,     1,     1,     1],\n",
      "        [    9,   389,  2082,  ...,     1,     1,     1]]), 'src_text': ['Though Levin had imagined that he was not hungry, and sat down to table only not to offend Kuzma, yet when he began eating he thought everything delicious.', 'But I was hurried on, and obeyed blindly the dictates of my fancy rather than my reason; and, accordingly, the ship being fitted out, and the cargo furnished, and all things done, as by agreement, by my partners in the voyage, I went on board in an evil hour, the 1st September 1659, being the same day eight years that I went from my father and mother at Hull, in order to act the rebel to their authority, and the fool to my own interests.', \"It's enough to drive one crazy!'\", '\"To a distance?\"', '\"O Miss Jane! don\\'t say so!\"', 'She answered his look with a smile.', 'This I know.\"', '\"I am tired, sir.\"'], 'tgt_text': ['Sebbene a Levin sembrasse di non aver appetito, e si fosse seduto a tavola solo per non dispiacere Kuz’ma, quando cominciò a mangiare, il pranzo gli parve straordinariamente gustoso.', 'Ma io fui affascinato; onde obbedii ciecamente ai dettati della mia fantasia anzichè a quelli della mia ragione. Per conseguenza, allestito il vascello, fornitone il carico, somministrato tutto quanto era fermato ne’ patti dalle parti interessate meco in tale viaggio, andai a bordo in trista ora al primo di settembre del 1659, lo stesso giorno in cui otto anni addietro fuggii da’ miei genitori ad Hull, ribellandomi alla loro autorità e facendomi giuoco del mio proprio vantaggio.', 'Mi farebbero diventar matta!”', '— Lontano?', '— Oh! signorina, non lo dite!', 'Lei sorrise al suo sguardo.', 'Lo so.', '— Sono stanca, signore.']}\n",
      "x.shape: torch.Size([8, 350, 512])\n",
      "x.shape[1]: 350\n",
      "self.pe:  torch.Size([1, 350, 512])\n",
      "self.pe[:, : x.shape[1], :]:  torch.Size([1, 350, 512])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "decoderoutput###################################\n",
      "decodig.............................................................................\n",
      "x.shape: torch.Size([8, 350, 512])\n",
      "x.shape[1]: 350\n",
      "self.pe:  torch.Size([1, 350, 512])\n",
      "self.pe[:, : x.shape[1], :]:  torch.Size([1, 350, 512])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch: 00:   0%|          | 1/3638 [00:03<3:50:58,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\t\t\t\tPadding added\n",
      "\t\t\t\tsequenceTextLimit: 350\n",
      "\t\t\t\tsrctextShape: torch.Size([350])\n",
      "\t\t\t\ttgttextShape: torch.Size([350])\n",
      "\t\t\t\tlabeltextShap: torch.Size([350])\n",
      "tensor([[[0, 1, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 1,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1],\n",
      "         [0, 0, 0,  ..., 1, 1, 1]]], dtype=torch.int32)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "batch: {'encoder_input': tensor([[   2,  100,  122,  ...,    1,    1,    1],\n",
      "        [   2,   12,  185,  ...,    1,    1,    1],\n",
      "        [   2, 5035,    7,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,   12,   88,  ...,    1,    1,    1],\n",
      "        [   2, 2495,    4,  ...,    1,    1,    1],\n",
      "        [   2,   62,  440,  ...,    1,    1,    1]]), 'decoder_input': tensor([[   2, 1674,   10,  ...,    1,    1,    1],\n",
      "        [   2,    9,  668,  ...,    1,    1,    1],\n",
      "        [   2, 1616, 5618,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    9,  249,  ...,    1,    1,    1],\n",
      "        [   2, 3016,    4,  ...,    1,    1,    1],\n",
      "        [   2, 2900,  353,  ...,    1,    1,    1]]), 'encoder_mask': tensor([[[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'decoder_mask': tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0]]]], dtype=torch.int32), 'label': tensor([[1674,   10,  104,  ...,    1,    1,    1],\n",
      "        [   9,  668,    4,  ...,    1,    1,    1],\n",
      "        [1616, 5618,   39,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   9,  249,  182,  ...,    1,    1,    1],\n",
      "        [3016,    4,  339,  ...,    1,    1,    1],\n",
      "        [2900,  353,  397,  ...,    1,    1,    1]]), 'src_text': ['She made my position much easier.', \"'Yes, of course that would have been better, Anna Arkadyevna,' replied the architect, 'but it's done now.'\", '_Nov._ 14, 15, 16.—These three days I spent in making little square chests, or boxes, which might hold about a pound, or two pounds at most, of powder; and so, putting the powder in, I stowed it in places as secure and remote from one another as possible.', 'Alice had no idea what to do, and in despair she put her hand in her pocket, and pulled out a box of comfits, (luckily the salt water had not got into it), and handed them round as prizes.', 'Yet people, merrily exchanging remarks, ran over the creaking boards of the platform, and the big station doors were constantly being opened and shut.', \"'It is possible to insult an honest man or an honest woman, but to tell a thief that he is a thief is only la constatation d'un fait.' [The statement of a fact.]\", \"Soon, Mary Vlasevna, soon... !'\", 'The case was thus: he had been with us now about a month, during which time I had let him see in what manner I had provided, with the assistance of Providence, for my support; and he saw evidently what stock of corn and rice I had laid up; which, though it was more than sufficient for myself, yet it was not sufficient, without good husbandry, for my family, now it was increased to four; but much less would it be sufficient if his countrymen, who were, as he said, sixteen, still alive, should come over; and least of all would it be sufficient to victual our vessel, if we should build one, for a voyage to any of the Christian colonies of America; so he told me he thought it would be more advisable to let him and the other two dig and cultivate some more land, as much as I could spare seed to sow, and that we should wait another harvest, that we might have a supply of corn for his countrymen, when they should come; for want might be a temptation to them to disagree, or not to think themselves delivered, otherwise than out of one difficulty into another.'], 'tgt_text': ['M’ha alleviato molto la situazione.', '— Già, s’intende, sarebbe stato meglio, Anna Arkad’evna — disse l’architetto — ma ormai è stato trascurato.', 'Dal 14 al 16. Passai quindi questi tre giorni facendo tante cassette o scatolette quadrate, ciascuna delle quali non portasse se non una libbra o due al più di polvere; indi posto in ciascuna di esse il suo carico le allogai in ripostigli sicuri e lontani quanto mi fu possibile l’uno dall’altro.', \"Alice non sapeva che fare, e nella disperazione si cacciò le mani in tasca, e ne cavò una scatola di confetti (per buona sorte non v'era entrata l'acqua,) e li distribuì in giro.\", 'Nel frattempo alcune persone corsero e, scambiando allegramente qualche parola, fecero scricchiolare le assi della banchina aprendo e richiudendo continuamente la porta grande.', '— Si può offendere un uomo onesto e una donna onesta, ma dire a un ladro che è un ladro è solo la constatation d’un fait.', 'Presto, presto, Lizaveta Petrovna.', 'Tanto meno essa sarebbe bastata ai suoi compagni, chè al suo dire ne viveano tuttavia sedici, se fossero capitati tutti ad un tratto. Meno poi ce n’era da potere fornir di vettovaglia un vascello che avremmo fabbricato per veleggiare a quale si fosse delle colonie cristiane in America.']}\n",
      "x.shape: torch.Size([8, 350, 512])\n",
      "x.shape[1]: 350\n",
      "self.pe:  torch.Size([1, 350, 512])\n",
      "self.pe[:, : x.shape[1], :]:  torch.Size([1, 350, 512])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "decoderoutput###################################\n",
      "decodig.............................................................................\n",
      "x.shape: torch.Size([8, 350, 512])\n",
      "x.shape[1]: 350\n",
      "self.pe:  torch.Size([1, 350, 512])\n",
      "self.pe[:, : x.shape[1], :]:  torch.Size([1, 350, 512])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing epoch: 00, Loss: 10.005:   0%|          | 0/3638 [00:03<?, ?it/s]\n",
      "Processing epoch: 00:   0%|          | 1/3638 [00:07<7:06:20,  7.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[900], line 94\u001b[0m\n\u001b[1;32m     92\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     93\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[900], line 71\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     68\u001b[0m writer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# backpropagate\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# update weihts\u001b[39;00m\n\u001b[1;32m     75\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from config import get_weights_file_path, get_config\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "def train_model(config):\n",
    "    # define device\n",
    "    device =  torch.device('cuda' if( torch.cuda.is_available) else 'cpu')\n",
    "    device =torch.device('cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # making sure weight folder is created\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "    print(f'config: {config}')\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard: to visualize loss\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    # optimize\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    \n",
    "    # resume training incase model crashes... restores stateof model and optimizer\n",
    "    initial_epoch=0\n",
    "    global_step=0\n",
    "    if(config['preload']):\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f\"Preloading model: {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        inital_epoch = state['epoch']+1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step=state['global_step']\n",
    "        \n",
    "    # loss fn \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1)\n",
    "    # ignore_index: To ignore padding tokens so that they don't have any impact on calculating loss\n",
    "    # Label smoothing is a technique used to smooth the target labels by assigning a small probability to the incorrect classes and reducing the confidence on the correct class. \n",
    "    # This helps prevent the model from becoming too confident and overfitting to the training data. \n",
    "    # label_smoothing=0.1 means that for each true label, 10% of the probability mass is redistributed to all other classes.\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during \n",
    "        # training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch: {epoch:02d}')\n",
    "        for batch in batch_iterator:\n",
    "            print(f\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nbatch: {batch}\")\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (batch_size, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (batch_size, seq_len)\n",
    "            encoder_mask= batch['encoder_mask'].to(device) # (batch_size,1,1, seq_len)\n",
    "            decoder_mask= batch['decoder_mask'].to(device) # (batch_size, 1 ,seq_len, seq_len)\n",
    "            \n",
    "            # run tensors through transformers\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask ) # (batch, seq_len, d_model)\n",
    "            print(\"decoderoutput###################################\")\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (batch, seq_len, d_model)\n",
    "            proj_ouput = model.project(decoder_output) # (batch, seq_len, tgt_vocab_size)\n",
    "            \n",
    "            # compare output with our labels\n",
    "            label = batch['label'].to(device)#(B, seq_len)\n",
    "            # (Batch, seq_len, tgt_vocab_size)  -->   (batch * seq_len, tgt_vocab_size)\n",
    "            loss = loss_fn(proj_ouput.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)) \n",
    "            # update progress bar\n",
    "            # batch_iterator.set_prefix(f\"loss: {loss.item():6.3f}\")\n",
    "            # batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch: {epoch:02d}, Loss: {loss.item():.3f}')\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            # log the loss\n",
    "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            # update weihts\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step+=1  # this is basically or tensorboard\n",
    "            \n",
    "        # save the model at end of every epoch\n",
    "        \n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        #  it is good ide to save state of model + optimizer\n",
    "        torch.save({\n",
    "            'epoch':epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'global_step':global_step\n",
    "            }, model_filename)\n",
    "            \n",
    "if __name__ =='__main__':\n",
    "    warnings.filterwarnings('ignore')\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss: 23123234234.000'"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=23123234234\n",
    "f\"loss: {num:6.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great question! Understanding the intuition behind the input shapes for the Cross-Entropy Loss (CE Loss) in the context of a sequence-to-sequence model, like a Transformer, is crucial for grasping how the model learns. Let's break down the reasoning behind the shapes of `proj_output` and `label` and how they interact during the loss computation.\n",
    "\n",
    "## Shapes of Inputs to Cross-Entropy Loss\n",
    "\n",
    "### 1. `proj_output`: Shape `(batch*seq_len, vocab_size)`\n",
    "\n",
    "- **Description**: This tensor contains the predicted logits for each token in the vocabulary for every position in the output sequences.\n",
    "- **Intuition**:\n",
    "  - **Batch and Sequence Flattening**: By reshaping the output to `(batch*seq_len, vocab_size)`, we are effectively treating each token prediction as an independent sample. This allows us to compute the loss for each predicted token across all sequences in the batch.\n",
    "  - **Logits for Each Token**: Each row in this tensor corresponds to a specific token position in the sequence (from the batch), and each column corresponds to the model's predicted score (logit) for each token in the vocabulary. The model outputs these logits before applying the softmax function to convert them into probabilities.\n",
    "\n",
    "### 2. `label`: Shape `(batch*seq_len)`\n",
    "\n",
    "- **Description**: This tensor contains the true labels (target tokens) for each position in the output sequences, flattened into a single dimension.\n",
    "- **Intuition**:\n",
    "  - **Single Token per Position**: Each entry in this tensor corresponds to the true token that should be predicted at that specific position in the sequence. By flattening the labels, we ensure that they align with the predictions in `proj_output`.\n",
    "  - **Direct Correspondence**: The label at each position directly corresponds to the predicted logits in `proj_output`. For example, if the first token in the output sequence is \"cat\", the label for that position will be the index of \"cat\" in the vocabulary.\n",
    "\n",
    "## Cross-Entropy Loss Calculation\n",
    "\n",
    "When you pass `proj_output` and `label` to the Cross-Entropy Loss function, here's what happens:\n",
    "\n",
    "1. **Softmax Application**: The CE Loss function applies the softmax operation to the logits in `proj_output` to convert them into probabilities. Each row (representing a specific token prediction) will sum to 1.\n",
    "\n",
    "2. **Loss Computation**: The loss function then compares the predicted probabilities against the true labels:\n",
    "\n",
    "   - For each position in the sequence, it calculates how well the predicted probabilities match the true label.\n",
    "   - It does this by taking the negative log probability of the true label (the index in the vocabulary) for each token position.\n",
    "\n",
    "3. **Average Loss**: The final loss value is typically averaged (or summed) over all the positions in the batch. This provides a single scalar value that represents how well the model is performing across the entire batch of sequences.\n",
    "\n",
    "## Summary\n",
    "\n",
    "The intuition behind reshaping `proj_output` to `(batch*seq_len, vocab_size)` and `label` to `(batch*seq_len)` is to enable a straightforward and efficient computation of the loss for each token in the sequences. By treating each token prediction as an independent sample, we can leverage the Cross-Entropy Loss to evaluate the model's performance on a token-by-token basis, facilitating effective learning during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind using the reshaped projected output tensor `(batch*seq_len, vocab_size)` and the reshaped target sequence tensor `(batch*seq_len)` as inputs to the cross-entropy loss function is to compare the predicted probability distribution over the vocabulary for each position in the sequence with the true label for that position.\n",
    "\n",
    "The projected output tensor `(batch*seq_len, vocab_size)` is a tensor that contains the predicted probability distribution over the vocabulary for each position in the sequence. Each row of the tensor corresponds to a position in the sequence, and each column corresponds to a word in the vocabulary. The value in each cell is the predicted probability of that word appearing at that position in the sequence.\n",
    "\n",
    "The target sequence tensor `(batch*seq_len)` is a tensor that contains the true label for each position in the sequence. Each element of the tensor is an integer that corresponds to the index of the true word in the vocabulary.\n",
    "\n",
    "The cross-entropy loss function calculates the loss between the predicted probability distribution and the true label for each position in the sequence. It does this by taking the negative log-likelihood of the true label given the predicted probability distribution. The loss is then averaged over all positions in the sequence to get the final loss value.\n",
    "\n",
    "By reshaping the projected output tensor and the target sequence tensor to have the same shape `(batch*seq_len)`, we can compare the predicted probability distribution and the true label for each position in the sequence. This allows us to update the model parameters in a way that minimizes the loss and improves the accuracy of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's an insightful observation. During inference, we indeed calculate the encoder output just once, and there are several important reasons for this:\n",
    "\n",
    "1. Efficiency:\n",
    "\n",
    "   - The encoder processes the input sequence (source language in translation tasks, for example).\n",
    "   - This input doesn't change during the generation of the output sequence.\n",
    "   - By computing it once and reusing it, we save significant computational resources.\n",
    "\n",
    "2. Model Architecture:\n",
    "\n",
    "   - In the encoder-decoder architecture (like in transformers), the encoder's role is to create a rich representation of the input sequence.\n",
    "   - This representation captures the meaning and context of the entire input.\n",
    "   - The decoder then uses this representation for each step of its output generation.\n",
    "\n",
    "3. Attention Mechanism:\n",
    "\n",
    "   - The decoder uses attention mechanisms to focus on different parts of the encoder output at each decoding step.\n",
    "   - Having the full encoder output available allows the decoder to attend to any part of the input at any time.\n",
    "\n",
    "4. Parallelization:\n",
    "\n",
    "   - The encoder can process the entire input sequence in parallel.\n",
    "   - This is more efficient than recomputing it at each step, especially for longer sequences.\n",
    "\n",
    "5. Consistency:\n",
    "\n",
    "   - Using the same encoder output for all decoding steps ensures that the model's understanding of the input remains consistent throughout the generation process.\n",
    "\n",
    "6. Memory Usage:\n",
    "\n",
    "   - While it might seem that storing the encoder output uses more memory, it's actually more memory-efficient than recomputing it at each step.\n",
    "\n",
    "7. Autoregressive Nature of Decoding:\n",
    "   - During inference, the decoder generates one token at a time, using previously generated tokens as input.\n",
    "   - The encoder output remains constant during this process, as it represents the fixed input sequence.\n",
    "\n",
    "In contrast, during training, we often have the full target sequence available, so we can process the entire decoder input in parallel. But during inference, we generate one token at a time, making multiple passes through the decoder while reusing the same encoder output.\n",
    "\n",
    "This approach of computing the encoder output once and reusing it is a key optimization in sequence-to-sequence models, particularly in tasks like machine translation, summarization, or any task where we generate a sequence based on a fixed input sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3,5).view(-1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 898,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3,5).view(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[899], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cls_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m sep_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cls_token_id, sep_token_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'translation'],\n",
       "    num_rows: 32332\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32332,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x['translation']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': '\"Jane, I don\\'t like cavillers or questioners; besides, there is something truly forbidding in a child taking up her elders in that manner.',\n",
       " 'it': '— Jane, non mi piace di essere interrogata. Sta male, del resto, che una bimba tratti così i suoi superiori.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['translation'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Jane, I don\\'t like cavillers or questioners; besides, there is something truly forbidding in a child taking up her elders in that manner.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['translation'][10]['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "362"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500 - len(x['translation'][10]['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tensor() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tensor() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "torch.tensor(x['translation'][10]['en'], torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noManEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
