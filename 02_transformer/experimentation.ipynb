{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. input embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 512]),\n",
       " tensor([[[-1.2200, -0.3931, -0.7504,  ...,  0.3975,  0.3986, -1.6671],\n",
       "          [ 1.1348,  1.4171, -1.4880,  ...,  0.0243,  0.7517, -0.7881],\n",
       "          [-0.4036,  1.6854, -0.3276,  ..., -0.2500,  1.5330, -0.2742],\n",
       "          [ 1.2500, -0.3054, -0.5367,  ..., -0.0130,  0.5787,  0.8706],\n",
       "          [ 1.2500, -0.3054, -0.5367,  ..., -0.0130,  0.5787,  0.8706]]],\n",
       "        grad_fn=<EmbeddingBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we;ll be building input embeddings\n",
    "# allows to convert token into embedding of dim 1x52  : token -> input ID(position in vocab) ->embedding\n",
    "\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            d_model (int): dim of vector\n",
    "            vocab_size (int): # of words in vocab\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "# Create an example input tensor (batch size 1, sequence length 5, embedding dimension 20)\n",
    "batch_of_sentences = torch.tensor([[5, 6, 7, 0, 0]])  # Shape: (batch_size, max_sentence_length)\n",
    "print(batch_of_sentences.shape)\n",
    "\n",
    "\n",
    "# Pass through the embedding layer\n",
    "# The forward method is called automatically when you use the instance like a function.\n",
    "embedded_sentences = input_embeddings(batch_of_sentences)\n",
    "embedded_sentences.shape, embedded_sentences  # (batch, seq_len, embedding dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(5, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "d_model = 6\n",
    "nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = InputEmbeddings(d_model=512, vocab_size=1000)\n",
    "x = torch.tensor([1,2,3,4])\n",
    "print(x.shape)\n",
    "input_embeddings(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch\n",
    "x = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "print(x.shape)\n",
    "input_embeddings(x).shape  #(batch_size, seq_len, d_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. positional encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([1, 5, 512])\n",
      "x.shape[1]: 5\n",
      "self.pe:  torch.Size([1, 15, 512])\n",
      "self.pe[:, : x.shape[1], :]:  torch.Size([1, 15, 512])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "input  tensor([[[-1.2200, -0.3931, -0.7504,  ...,  0.3975,  0.3986, -1.6671],\n",
      "         [ 1.1348,  1.4171, -1.4880,  ...,  0.0243,  0.7517, -0.7881],\n",
      "         [-0.4036,  1.6854, -0.3276,  ..., -0.2500,  1.5330, -0.2742],\n",
      "         [ 1.2500, -0.3054, -0.5367,  ..., -0.0130,  0.5787,  0.8706],\n",
      "         [ 1.2500, -0.3054, -0.5367,  ..., -0.0130,  0.5787,  0.8706]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "input shape torch.Size([1, 5, 512])\n",
      "positional_encoded shape torch.Size([1, 5, 512])\n",
      "tensor([[[-2.4399,  1.2139, -1.5008,  ...,  2.7950,  0.0000, -1.3342],\n",
      "         [ 3.9525,  3.9149, -1.3322,  ...,  2.0486,  1.5037,  0.0000],\n",
      "         [ 0.0000,  2.5385,  1.2177,  ...,  1.5000,  0.0000,  0.0000],\n",
      "         [ 0.0000, -2.5909, -0.5833,  ...,  0.0000,  1.1580,  3.7413],\n",
      "         [ 0.9864, -1.9182, -2.3878,  ...,  1.9741,  1.1582,  3.7413]]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        \"\"\"\n",
    "                Since our model contains no recurrence and no convolution, in order for the model to make use of the\n",
    "        order of the sequence, we must inject some information about the relative or absolute position of the\n",
    "        tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\n",
    "        bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\n",
    "        as the embeddings, so that the two can be summed. There are many choices of positional encodings,\n",
    "        learned and fixed [9].\n",
    "        In this work, we use sine and cosine functions of different frequencies:\n",
    "            `PE(pos,2i) = sin(pos/(10000)**2i/dmodel)`\n",
    "            `PE(pos,2i+1) = cos(pos/(10000)**2i/dmodel)`\n",
    "        where pos is the position and i is the dimension. That is, each dimension of the positional encoding\n",
    "        corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\n",
    "        chose this function because we hypothesized it would allow the model to easily learn to attend by\n",
    "        relative positions, since for any fixed offset k, P E(pos+k) can be represented as a linear function of\n",
    "        PE(pos).\n",
    "\n",
    "        Keyword arguments:\n",
    "        dropout -- to make model less overfit\n",
    "        seq_len -- Specifies the maximum length of sequence that the model can handle. This helps determine the scale and range of the positional encodings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encodeing shape: seq_len X d_model i.e. each token will be represented (1*d_model) vector\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        formula :`PE(pos,2i) = cos(pos/(10000)**2i/dmodel) for i=1,3,5, ...and `PE(pos,2i) = sin(pos/(10000)**2i/dmodel) for i=2,4,6, ...and `\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        #  Create a model of shape (seq_len , d_model)\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        #  create a vector of shape(seq_len,1) to represent position of word in sequence\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)  # (seq_len,1)  # pos in formula\n",
    "        # create denominator of formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # apply sin to even positions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # apply cos to odd positions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # now we need to add batch dimension to these sentences so we can apply it to whole sentences, so to all the batch of sentence, because weill have batch of sentences.\n",
    "        # adding batch dim\n",
    "        pe = pe.unsqueeze(0)  # (1, seq_len, d_model)\n",
    "\n",
    "        # register this tensor in buffer of module  .. it is done for the tensor that you want to keep inside the module, not as a lerarned parameter but you want it to be saved when you save the file of the model\n",
    "        # you should register it as a buffer. this way the tensor would be saved in file along with state of model\n",
    "        self.register_buffer(\"pe\", pe)  # This is typically used to register a buffer that should not to be considered a model parameter.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Say you have a linear layer nn.Linear. You already have weight and bias parameters. But if you need a new parameter you use register_parameter() to register a new named parameter that is a tensor.\n",
    "        When you register a new parameter it will appear inside the module.parameters() iterator, but when you register a buffer it will not.\n",
    "        The difference:\n",
    "        Buffers are named tensors that do not update gradients at every step, like parameters. For buffers, you create your custom logic (fully up to you).\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        we need to add positional encoding to every token/word inside sequence/sentence\n",
    "        \"\"\"\n",
    "        print(\"x.shape:\",x.shape)\n",
    "        print(\"x.shape[1]:\",x.shape[1])\n",
    "        print(\"self.pe: \", self.pe.shape)\n",
    "        print(\"self.pe[:, : x.shape[1], :]: \", self.pe[:, :, :].shape)\n",
    "        print(\"\\n\\n\\n\\n\")\n",
    "        \n",
    "        \n",
    "        x = x + (self.pe[:, : x.shape[1], :]).requires_grad_(False)  # x:token and pe is positional encoding  # because we dont want to learn pe because these are fixed\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "positional = PositionalEncoding(d_model=512, seq_len=15, dropout=0.5)\n",
    "\n",
    "# Create an example input tensor (batch size , sequence length , embedding dimension )\n",
    "\n",
    "# Apply positional encoding\n",
    "positional_encoded = positional(embedded_sentences)\n",
    "print(\"input \", embedded_sentences)\n",
    "\n",
    "print(\"input shape\", embedded_sentences.shape)\n",
    "print(\"positional_encoded shape\", positional_encoded.shape)\n",
    "\n",
    "print(positional_encoded)  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 10]),\n",
       " tensor([[-1.4844,  0.4933, -0.0036,  0.6359, -0.8799,  0.3829,  1.3949, -0.1755,\n",
       "          -0.5400, -0.4064]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tensor\n",
    "tensorx = torch.randn(10,10)\n",
    "tensory = torch.randn(1, 10)\n",
    "tensorx.shape, tensory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 10]),\n",
       " tensor([[-5.9763e-01,  2.6529e-01,  1.7974e+00, -6.6407e-01,  4.6323e-02,\n",
       "           7.7210e-01, -3.9345e-01, -1.3822e+00, -2.4659e-01, -6.2363e-01],\n",
       "         [-4.3581e-01, -1.0833e+00,  1.3641e+00, -7.5099e-01, -9.4403e-01,\n",
       "           8.2546e-01,  3.8364e-01,  2.0075e+00, -7.5818e-01,  2.3921e+00],\n",
       "         [-2.0110e-01, -4.1344e-01, -1.1448e-01,  2.0815e-02,  3.6489e-01,\n",
       "           1.2361e+00,  1.2778e+00,  9.4924e-01, -1.0368e-01, -1.1758e+00],\n",
       "         [-8.2014e-01, -1.9384e-01,  4.4325e-02,  9.3753e-01,  4.1786e-01,\n",
       "           1.2542e+00, -5.2458e-01, -1.0932e+00, -2.0895e+00, -1.9191e+00],\n",
       "         [ 1.2966e+00,  5.8437e-01, -1.3983e+00, -1.4323e+00, -1.2751e+00,\n",
       "           9.2704e-04,  2.5904e-01,  1.1686e-01,  1.7038e-01,  4.4929e-01]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorx[::2,:].shape, tensorx[::2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 2]),\n",
       " tensor([[-5.9763e-01,  7.7210e-01],\n",
       "         [-4.3581e-01,  8.2546e-01],\n",
       "         [-2.0110e-01,  1.2361e+00],\n",
       "         [-8.2014e-01,  1.2542e+00],\n",
       "         [ 1.2966e+00,  9.2704e-04]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorx[::2,::5].shape, tensorx[::2,::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 register_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModule()\n",
      "Buffer tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Another buffer tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "\n",
    "        # Register a buffer tensor with zeros\n",
    "        self.register_buffer(\"buffer_tensor\", torch.zeros(3, 3))\n",
    "\n",
    "        # Register another buffer tensor with specific values\n",
    "        data = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)\n",
    "        self.register_buffer(\"another_buffer\", data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use the buffer tensors in the forward pass\n",
    "        output = x + self.buffer_tensor\n",
    "        return output\n",
    "\n",
    "\n",
    "# Create an instance of MyModule\n",
    "model = MyModule()\n",
    "\n",
    "# Print the module to see its structure\n",
    "print(model)\n",
    "\n",
    "# Accessing the buffer tensors\n",
    "print(\"Buffer tensor:\")\n",
    "print(model.buffer_tensor)\n",
    "\n",
    "print(\"\\nAnother buffer tensor:\")\n",
    "print(model.another_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffers in PyTorch are a key feature that allows you to store tensors that are not parameters of a model but are still important for its operation. Here's an in-depth look at buffers, including their purpose, usage, and how they differ from parameters.\n",
    "\n",
    "## What are Buffers?\n",
    "\n",
    "Buffers are named tensors that are registered within a PyTorch `nn.Module`. Unlike model parameters, which are learned during training (e.g., weights and biases), buffers are typically used to store state information that should be preserved across model saves and loads, but does not require gradients.\n",
    "\n",
    "### Key Characteristics of Buffers\n",
    "\n",
    "1. **Non-learnable**: Buffers do not get updated during the training process through backpropagation. They are not optimized by any learning algorithm.\n",
    "2. **State Preservation**: Buffers are saved in the model's `state_dict`, allowing them to be restored when the model is loaded. This is useful for maintaining certain statistics or configurations that are necessary for the model's operation.\n",
    "\n",
    "3. **Device Management**: Buffers are automatically moved to the appropriate device (CPU or GPU) when the model is transferred between devices using `.to()`, ensuring that they are always in the correct context for computations.\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "Buffers are often used in scenarios where you need to maintain certain statistics or configurations that evolve over time but are not directly learned parameters. A common example is in batch normalization, where the running mean and variance are stored as buffers.\n",
    "\n",
    "```python\n",
    "class RunningMean(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.9):\n",
    "        super().__init__()\n",
    "        self.momentum = momentum\n",
    "        self.register_buffer('mean', torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Update the running mean\n",
    "        self.mean = self.momentum * self.mean + (1 - self.momentum) * x.mean(dim=0)\n",
    "        return x\n",
    "```\n",
    "\n",
    "In this example, `mean` is a buffer that keeps track of the running average of the input features.\n",
    "\n",
    "### Registering Buffers\n",
    "\n",
    "You can register a buffer in a PyTorch module using the `register_buffer()` method. This method takes the name of the buffer and the tensor you want to register.\n",
    "\n",
    "```python\n",
    "self.register_buffer('buffer_name', tensor)\n",
    "```\n",
    "\n",
    "### Differences Between Buffers and Parameters\n",
    "\n",
    "1. **Gradient Tracking**: Parameters are part of the model's learnable parameters and have gradients computed for them during training. Buffers do not have gradients and are not updated through optimization algorithms.\n",
    "\n",
    "2. **Inclusion in `parameters()`**: When you call the `parameters()` method on a module, it returns only the parameters. Buffers will not be included in this list.\n",
    "\n",
    "3. **Use Cases**: Buffers are typically used for maintaining state (like running statistics), while parameters are used for weights and biases that need to be learned from data.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- **Memory Management**: Buffers can help manage memory by storing intermediate computations or states that are needed later without the overhead of learning them.\n",
    "- **Checkpointing**: Buffers can be used in techniques like checkpointing, where you save the state of certain variables without saving the entire model, allowing for more efficient training and inference.\n",
    "\n",
    "- **Integration with Other Features**: Buffers can be used alongside other PyTorch features like hooks, which allow for custom operations during the forward and backward passes of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameters automatically get moved to gpu with .to(device) but tensor dont that why we have to register them as buffer\n",
    "\n",
    "Yes, you are correct. When you move a PyTorch model to a device (e.g., a GPU) using .to(device), the model’s parameters are automatically moved to the specified device. However, regular tensors defined within the model are not automatically moved. To ensure that these tensors are moved along with the parameters, you register them as buffers.\n",
    "\n",
    "### Why Use Buffers?\n",
    "\n",
    "1. Automatic Device Management: Buffers are automatically moved to the specified device when you call .to(device) on the model.\n",
    "2. Consistency: Buffers maintain consistency in terms of device allocation, ensuring all parts of the model (parameters and non-parameters) are on the same device.\n",
    "3. Persistence: Buffers are saved and loaded along with the model’s parameters, making it easier to manage the model state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter device: cuda:0\n",
      "Regular tensor device (not moved): cpu\n",
      "Buffer device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.param = nn.Parameter(torch.randn(3, 3))  # A learnable parameter\n",
    "        self.buffer = torch.randn(3, 3)  # A regular tensor\n",
    "        self.register_buffer('registered_buffer', torch.randn(3, 3))  # A registered buffer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.param + self.registered_buffer\n",
    "\n",
    "# Create an instance of the module\n",
    "model = MyModule()\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Check device of model components\n",
    "print(\"Parameter device:\", model.param.device)\n",
    "print(\"Regular tensor device (not moved):\", model.buffer.device)\n",
    "print(\"Buffer device:\", model.registered_buffer.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add & Norm - layer normalization\n",
    "\n",
    "for each item in the batch, calculte mean & var, and normalize each item so that each has mean=0, and var of 1(z-standardization), Beta and Gamma are also learnt to minimize the data flactuation as having values between - and 1 might be too restrictive.\n",
    "\n",
    "new xj = (xj -meanj) / math.sqrt(var\\*\\*2 + epsilon)\n",
    "\n",
    "simplified version: `x = α * (x - μ) / (σ + ε) + β`\n",
    "\n",
    "gamma(multiplication) and beta(addition) will be learnt after this. epsilon is for numericalsatability as if denominator gets very small, overall number would be difficult to manage percision wise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2860,  1.5826,  0.8467,  ...,  0.2109,  1.4458,  1.1176],\n",
       "         [-0.3088, -2.4583, -0.6232,  ..., -0.3088, -0.3088, -0.1993],\n",
       "         [-0.3398,  0.0206, -0.3398,  ..., -0.3398,  1.0731, -0.3398],\n",
       "         [ 1.1861,  0.6665, -0.9731,  ...,  0.7069, -0.3394, -0.3394],\n",
       "         [-0.3012,  1.1046, -2.0323,  ..., -0.3012, -0.3012,  1.9546]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 10**-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps  # epsilon\n",
    "        self.alpha = nn.Parameter(torch.ones(1))  # gamma  # mulltiplied\n",
    "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        # print(\"mean shape\", mean.shape, mean)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
    "\n",
    "\n",
    "ln = LayerNormalization()\n",
    "\n",
    "# print(\"Before normalization:\")\n",
    "# print(positional_encoded)\n",
    "\n",
    "normalized = ln(positional_encoded)\n",
    "# print(\"After normalization:\")\n",
    "print(normalized.shape)\n",
    "normalized  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. feed forward block\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\n",
    "connected feed-forward network, which is applied to each position separately and identically. This\n",
    "consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "`FFN(x) = max(0, xW1 + b1)W2 + b2 (2)` # two lyers with ReLu in between\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters\n",
    "from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
    "The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
    "dff = 2048.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before feedforwardblock:\n",
      "torch.Size([1, 5, 512]) tensor([[[ 0.2860,  1.5826,  0.8467,  ...,  0.2109,  1.4458,  1.1176],\n",
      "         [-0.3088, -2.4583, -0.6232,  ..., -0.3088, -0.3088, -0.1993],\n",
      "         [-0.3398,  0.0206, -0.3398,  ..., -0.3398,  1.0731, -0.3398],\n",
      "         [ 1.1861,  0.6665, -0.9731,  ...,  0.7069, -0.3394, -0.3394],\n",
      "         [-0.3012,  1.1046, -2.0323,  ..., -0.3012, -0.3012,  1.9546]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 5, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4898, -0.6100,  0.5276,  ...,  0.2296,  0.1190,  0.1357],\n",
       "         [-0.4612, -0.0627,  0.0864,  ...,  0.0666,  0.2540,  0.2654],\n",
       "         [-0.3004, -0.2013,  0.4908,  ...,  0.3137, -0.1830,  0.2191],\n",
       "         [-0.5116, -0.4162,  0.2457,  ..., -0.2102,  0.2946, -0.1691],\n",
       "         [-0.1928, -0.1139,  0.1022,  ...,  0.7609,  0.2351,  0.1679]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff, bias=True)  # first layer: w1,b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model, bias=True)  # second layer: w2,b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input:(batch, seq_len, d_model)\n",
    "\n",
    "        # after first layer: (batch, seq_len, d_ff)\n",
    "\n",
    "        # after second layer: (batch, seq_len, d_model)\n",
    "\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "\n",
    "\n",
    "feedforwardblock = FeedForwardBlock(d_model=512, d_ff=2048, dropout=0.5)\n",
    "\n",
    "print(\"Before feedforwardblock:\")\n",
    "print(normalized.shape, normalized)\n",
    "\n",
    "feedforwarded = feedforwardblock(normalized)\n",
    "# print(\"After normalization:\")\n",
    "print(feedforwarded.shape)\n",
    "feedforwarded  # (1, seq_len,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multi-head attention\n",
    "\n",
    "takes input:(seq_len, d_model) of encoder and uses it three times k:key, q:query, v:values. then we multiply these matrices with Wk, Wq and Wv respectively. resulting in K',Q',V' of same(seq_len, d_model) dim. Now,split each of K', Q' and V' into h parts along d_model(embedding) dim where h is number of head. So that each head will have access to full sentence but different part of embedding of each token.\n",
    "\n",
    "Now, apply following formulas to each head which will result into h matrices of `(seq_len, d_k)` dims where `d_k` = `d_model/h`\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW^Q_i, K W^K_i, V W^V_i)\n",
    "$$\n",
    "\n",
    "Now concatenate all heads,\n",
    "\n",
    "$$\n",
    "\\text{MultiHead(Q, K, V)} = \\text{Concatenate}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h) W^o\n",
    "$$\n",
    "\n",
    "![alt text](02_transformer/MHA.png)\n",
    "\n",
    "W^o is of `(seq, h*d_v)` shape where `d_v = d_k`\n",
    "\n",
    "and resultant MH-A is `(seq_len, d_model)` same as input\n",
    "\n",
    "But we also have to consider batch_dim for dealing with multiple sentences; the above intition works for single sentence.\n",
    "\n",
    "`SO WE WILL CONSIDER BATCH DIMENSION.`\n",
    "\n",
    "---\n",
    "\n",
    "### MASK\n",
    "\n",
    "before applying multiplying with V meaning\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "we get a scaled dot profuct of q and k, its (seq_len, seq_len) matrix. this shows interaction of each words with each other word.\n",
    "\n",
    "If we dont want some words to interact with other words, we basically replace there attention score(before applying softmax) with very small value, which means after softmax these values will become zero,so basically we hide attention between those two words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.dropout = dropout\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.d_k = d_model // h\n",
    "\n",
    "        # define weight matrices\n",
    "        self.wq = nn.Linear(d_model, d_model)  # wq\n",
    "        self.wk = nn.Linear(d_model, d_model)  # wk\n",
    "        self.wv = nn.Linear(d_model, d_model)  # wv\n",
    "\n",
    "        # output matrix Wo (h*dv, d_model) where dv = dk\n",
    "        self.wo = nn.Linear(d_model, d_model)  # wo\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod  # so we could cal fn wothout specifying class instance\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]  # last dim of query/key/value\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> # (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)  # transpose(-2, -1): transpose last two dims\n",
    "\n",
    "        # apply mask: just replace values you want to mask with very small values\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask == 0, -1e9)  # replace all values where mask==0 (conidtion is true) with -1e9\n",
    "\n",
    "        # applying softmax\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (batch_size, h, seq_len,seq_len)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        query = self.wq(q)\n",
    "        print(\"q::::::::::::::::::::::::::::::::::::::::::::::::::::\", q.shape)\n",
    "        print(\"k::::::::::::::::::::::::::::::::::::::::::::::::::::\", k.shape)\n",
    "        print(\"v::::::::::::::::::::::::::::::::::::::::::::::::::::\", v.shape)\n",
    "\n",
    "        key = self.wk(k)\n",
    "        value = self.wv(v)\n",
    "\n",
    "        # splitting\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, h, d_k) -> (batch, h, seq_len, d_k)\n",
    "        # we moved h dimension because we want each head to consider (seq_len, d_k)\n",
    "        # each head considers full sentence but smaller embedding\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # (batch, h, seq_len, d_k)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query=query, key=key, value=value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # (batch, h, seq_len, d_k) -> (batch, seq_len, h, d_k)\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        # (batch, seq_len, d_model)\n",
    "        return self.wo(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Residual/skip connection\n",
    "\n",
    "between add & Norm and previous layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):  # sublayer:previous layer\n",
    "        \"\"\"take x and combine with output of next layer\"\"\"\n",
    "\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoderblock\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n",
    "\n",
    "it will contain one multi-head attention, two Add&Norm, one Feed forward block and two residual connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(2)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        src_mask: mask we want to apply to input of encoder. we need this to hide interaction of padding word with other words.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # multihead attention within\n",
    "\n",
    "        x = self.residual_connection[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
    "\n",
    "        \"\"\"\n",
    "        The lambda is used because self_attention_block needs four arguments (query, key, value, mask) \n",
    "        but ResidualConnection expects a function that takes only one argument.\n",
    "        The lambda allows us to create a function that takes one argument x and expands it to the required four arguments, including the src_mask.\n",
    "        \"\"\"\n",
    "        x = self.residual_connection[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "is made up of many encoder\n",
    "\n",
    "Each encoder block is repeated Nx times\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 14-45-21.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers: nn.ModuleList, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "        self.norm = LayerNormalization()  # at end\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # apply on layer after another # order matters\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-22 15-50-17.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output embeddings\n",
    "\n",
    "output embeddings are same as input embeddings, so weill just intialize it twice\n",
    "\n",
    "masked attention is some what same as self attenntion because of 3 same inputs while Mulihead attention block is actually cross attension bea=cause key and value are cping from encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "\n",
    "        # we have three residual connections\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout=dropout) for _ in range(3)])\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        Args:\n",
    "            x: input of decoder\n",
    "            src_mask: mask applied to encoder\n",
    "            tgt_mask: target mask applied to decoder\n",
    "\n",
    "\n",
    "\n",
    "        src_mask and tgt_mask because we are dealing with language transalation. SO, source language is English and target language is italian\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # i. masked multihead attention: first residual connection\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))  # tgt_mask:becasue its decoder\n",
    "\n",
    "        # ii. cross attention: second residual connection\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"build decoder which is n times DecoderBlock one after anotherjust we did for encoder\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): whic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            # each layer is a decoderblock\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection/linear layer\n",
    "\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-23 10-50-34.png>)\n",
    "\n",
    "output of multihead attention is (seq_len, d_model)\n",
    "\n",
    "However we want to these words back into vocabularly which convert embedding to position in vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_moel: int, vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        this is a linear layer that is converting from d_model to vocab_size\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch seq_len, vocab_size)\n",
    "        # The purpose of applying softmax is to convert the raw output of the linear layer into a probability distribution over the vocabulary.\n",
    "        #  we will also apply softmax, specifically log_softmax for mathematiacal stability\n",
    "\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        \"\"\"\n",
    "        we need source embedding and target embedding because we are dealing with multiple languages\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # three methods, one to encoder, one to decode and one to project\n",
    "    # Not creating single forward method because we can reuse output of encoder and to also visualize the attention\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            src (_type_): src of language\n",
    "            src_mask (_type_): source mask\n",
    "        \"\"\"\n",
    "\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        src = self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_ouput, src_mask, tgt, tgt_mask):\n",
    "        print(\"decodig.............................................................................\")\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        return self.decoder(tgt, encoder_ouput, src_mask, tgt_mask)\n",
    "\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "    \"\"\"sumary_line\n",
    "\n",
    "    we need vocab size of src and tgt so get info about how many vectors to be created\n",
    "    Keyword arguments:\n",
    "\n",
    "    N: number of input layers i.e. number of enccoder blocks and number of decoder blocks\n",
    "    h: # of heads\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"strcuture will be same across all tasks\"\"\"\n",
    "\n",
    "    # create embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n",
    "\n",
    "    # positional encoding layers\n",
    "    # one encoding layer wold be enough\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "\n",
    "    # create encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "    # create encoder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # now create encoder and decoder\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # create projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "    # create transfromer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # initilize parameter to make trainig faster so they dont just strat with random values\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# engIt - task\n",
    "\n",
    "1. Download dataset: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it\n",
    "2. build tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer: create sentence into tokens. h=there are manyh tokenizers like BPE, subword-level,, word level, etc\n",
    "# we'll be creting word-level tokenizer i.e. split by space\n",
    "# so tokenizer builds vocab and maps tokens to index\n",
    "# there would be special tokens too for paddings, start of sentence, end of sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train,py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zohaib/anaconda3/envs/noManEnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "\n",
    "# class that will train tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace  # to split words according to whitespace\n",
    "\n",
    "from pathlib import Path  # to assist in creating absolutes path using relative paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    # pasrsing each item which is a pair in dataset # (english, italian)\n",
    "    for item in ds:\n",
    "        yield item[\"translation\"][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buils the tokeizer\n",
    "def get_or_build_tokenizer(config, ds, lang):\n",
    "    \"\"\"\n",
    "    Building the tokenizer\n",
    "\n",
    "    Keyword arguments:\n",
    "    config -- config of our model\n",
    "    ds -- dataset\n",
    "    lang -- lang to build tokeizer for\n",
    "    \"\"\"\n",
    "\n",
    "    # file to save this tokenizer\n",
    "    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))  # mean we can change\n",
    "    #'../tokenizers/tokenizer_{0}.format(lang).json' same as f\"'../tokenizers/tokenizer_{lang}'\n",
    "\n",
    "    # so if tokenizer ddoesn't exists we create it\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()  # split by whitespces\n",
    "        #  now training tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)  # for a word to appear in vocab it must have min frequency of 2\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        print(tokenizer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and get tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# from dataset import BilingualDataset, causal_mask\n",
    "\n",
    "\n",
    "def get_ds(config):\n",
    "    ds_raw = load_dataset(\"opus_books\", f\"{config['lang_src']}-{config['lang_tgt']}\", split=\"train\")\n",
    "    # build tokenizer\n",
    "    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n",
    "\n",
    "    # keep 10% for val, 90 for trainig ... hf dst has single split so we'll be splitting manually\n",
    "    train_ds_size = int(0.9 * len(ds_raw))\n",
    "    val_ds_size = len(ds_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n",
    "\n",
    "    #\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # max sequence length of src and tgt in both splits\n",
    "    max_len_src, max_len_tgt = 0, 0\n",
    "\n",
    "    for item in ds_raw:\n",
    "        # load each sentence , convert it to ids using tokenizer and i check length.\n",
    "        src_ids = tokenizer_src.encode(item[\"translation\"][config[\"lang_src\"]]).ids\n",
    "        tgt_ids = tokenizer_tgt.encode(item[\"translation\"][config[\"lang_tgt\"]]).ids\n",
    "\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f\"Max length of src sentence{max_len_src}\")\n",
    "    print(f\"Max length of tgt sentence{max_len_tgt}\")\n",
    "\n",
    "    # data loader\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)  # batch_size=1 because we want to process each sentence one by one\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import build_transformer\n",
    "\n",
    "\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        config (_type_): _description_\n",
    "        vocab_src_len (_type_): source vocab size\n",
    "        vocab_tgt_len (_type_): target vocab size\n",
    "    \"\"\"\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config[\"seq_len\"], config[\"d_model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # saving particular tokens to create the tensor for the model.so we need SOS, EOS, and PAD tokens so how are we going to assign ID to these tokens?\n",
    "        self.sos_token = torch.tensor([tokenizer_src.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_src.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_src.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "    # length of this dataset\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # extracting original pair from hfDataset\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair[\"translation\"][self.src_lang]\n",
    "        tgt_text = src_target_pair[\"translation\"][self.tgt_lang]\n",
    "\n",
    "        # converting text into IDs\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        #  adding padding to reach `seq_length` because model works with fixed length\n",
    "        # adding padding tokens\n",
    "\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # -2 for sos and eos token\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1  # -1 for sos token  # eos will be generated by model\n",
    "        \"\"\"\n",
    "        while training,on decode side, we only add SOS and on label side we add EOS token.\n",
    "        so for dec_num_padding_tokens we only need to add one of special tokens\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure our seq_len is enough to represent all of sentences in our dataset. i.e. padding must never become negative\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "\n",
    "        # lets build to tensors for encoder input and decoder input and also for lebel. so, one sentence would be send to encoder,\n",
    "        # one to decoder input and one that we expect as decoder's output and that output will be called label/target\n",
    "\n",
    "        # encoder input = sos + src_text + eos + pad_tokens\n",
    "        encoder_input = torch.cat([self.sos_token, torch.tensor(enc_input_tokens, dtype=torch.int64), self.eos_token, torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)])\n",
    "\n",
    "        # decoder input = sos + decoder input + padtokens\n",
    "        decoder_input = torch.cat([self.sos_token, torch.tensor(dec_input_tokens, dtype=torch.int64), torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)])\n",
    "\n",
    "        # label: decoder output that we expect=  decoder input +eos+ padtokens (add EOS to label)\n",
    "        label = torch.cat([torch.tensor(dec_input_tokens, dtype=torch.int64), self.eos_token, torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)])\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        \"\"\"In encoder inout, we are adding padding to match length but we dont want these paddings to effect in attenation mechanism so we'll mask these paddings \n",
    "        that will indicate that these tokens shouldn't be considered: encoder_mask\n",
    "        \n",
    "        it says all the tokens that are not padding are OK\n",
    "        \n",
    "        -------\n",
    "        \n",
    "        for decoder we need a specail mask calledcausal mask which means each word can only look at previous token and non-padding token. Remember we only want real words to be considred\n",
    "        in self attention.\n",
    "        \n",
    "        encoder_input: (seq,_len)  \n",
    "        Decoder_input: (seq,_len)  \n",
    "        encoder_mask: (1, batch, seq,_len)  \n",
    "        decoder_mask: (1,batch, seq_len) & (1,seq_len, seq_len)\n",
    "        label:(seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": encoder_input,\n",
    "            \"decoder_input\": decoder_input,\n",
    "            \"encoder_mask\": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            \"decoder_mask\": (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            \"label\": label,\n",
    "            \"src_text\": src_text,\n",
    "            \"tgt_text\": tgt_text,\n",
    "        }  # (seq,_len)  # (seq,_len)  # (1, batch, seq,_len)  # causal_mask will build matrix of seq_len * seq_len     # (1,batch, seq_len) & (1,seq_len, seq_len)\n",
    "\n",
    "\n",
    "def causal_mask(size):\n",
    "    \"\"\"\n",
    "    causal_mask will build matrix of seq_len * seq_len     # (1,batch, seq_len) & (1,seq_len, seq_len)\n",
    "    we want each word in decider to only watch non padding words that come before it.\n",
    "    Below matrix represent K\\*Q in softmax attention, we want to hide all the values above the diagonal.\n",
    "    so we want all values above diagonal to be masked out.\"\"\"\n",
    "\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True, False, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right to focus on this part. It's a crucial detail in sequence-to-sequence models like those used for machine translation. Let's break this down further:\n",
    "\n",
    "1. Encoder (Source Language) Side:\n",
    "\n",
    "   - We add both SOS (Start of Sequence) and EOS (End of Sequence) tokens.\n",
    "   - SOS tells the model \"The sentence is starting here.\"\n",
    "   - EOS tells the model \"The sentence ends here.\"\n",
    "   - This helps the encoder to understand the full context of the input sentence.\n",
    "\n",
    "2. Decoder (Target Language) Side:\n",
    "   - During training, we only add the SOS token at the beginning.\n",
    "   - The EOS token is not added to the input, but is expected in the output.\n",
    "\n",
    "Why this difference?\n",
    "\n",
    "1. For the Encoder:\n",
    "\n",
    "   - The full context is important. The model needs to know where the sentence starts and ends to encode all the information correctly.\n",
    "\n",
    "2. For the Decoder:\n",
    "   - During training, the decoder is typically fed the correct translation one word at a time (a technique called teacher forcing).\n",
    "   - It starts with SOS to know when to begin generating the translation.\n",
    "   - It should learn to generate EOS when it thinks the translation is complete.\n",
    "   - By not providing EOS in the input but expecting it in the output, we're teaching the model to decide when to stop generating.\n",
    "\n",
    "Practical example:\n",
    "\n",
    "Let's say we're translating \"Hello, how are you?\" from English to French.\n",
    "\n",
    "Encoder input might look like:\n",
    "[SOS] Hello , how are you ? [EOS]\n",
    "\n",
    "Decoder input during training might look like:\n",
    "[SOS] Bonjour , comment allez - vous ?\n",
    "\n",
    "And the expected output (label) would be:\n",
    "Bonjour , comment allez - vous ? [EOS]\n",
    "\n",
    "This way, the model learns to:\n",
    "\n",
    "1. Understand complete sentences (encoder)\n",
    "2. Start generating translations (decoder input)\n",
    "3. Know when to stop generating (decoder output)\n",
    "\n",
    "This subtle difference is key to training a model that can both understand full sentences and generate complete translations of varying lengths.\n",
    "\n",
    "### causal Mask\n",
    "\n",
    "we want each word in decider to only watch non padding words that come before it.\n",
    "Below matrix represent K\\*Q in softmax attention, we want to hide all the values above the diagonal.\n",
    "so we want all values above diagonal to be masked out.\n",
    "![alt text](<02_transformer/Screenshot from 2024-07-29 10-43-45.png>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1, 2, 3]) != 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(1, 10, 10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 20,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"lang_src\": \"en\",\n",
    "        \"lang_tgt\": \"it\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": None,  # preload to restart training if crashed\n",
    "        \"tokenizer_file\": \"tokenizer{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\",\n",
    "    }\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config[\"model_folder\"]\n",
    "    model_basename = config[\"model_basename\"]\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\"\n",
    "    return str(Path(\".\") / model_folder / model_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Max length of src sentence309\n",
      "Max length of tgt sentence274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch: 00:   0%|          | 0/3638 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "decoderoutput###################################\n",
      "decodig.............................................................................\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "k:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "v:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n",
      "q:::::::::::::::::::::::::::::::::::::::::::::::::::: torch.Size([8, 350, 512])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     88\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 52\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     50\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(encoder_input, encoder_mask ) \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoderoutput###################################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m proj_ouput \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mproject(decoder_output) \u001b[38;5;66;03m# (batch, seq_len, tgt_vocab_size)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# compare output with our labels\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 36\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, encoder_ouput, src_mask, tgt, tgt_mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_embed(tgt)\n\u001b[1;32m     35\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_pos(tgt)\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_ouput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, encoder_output, src_mask, tgt_mask):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m# each layer is a decoderblock\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, encoder_output, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_block(x, x, x, tgt_mask))  \u001b[38;5;66;03m# tgt_mask:becasue its decoder\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ii. cross attention: second residual connection\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_connections\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m2\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mResidualConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):  \u001b[38;5;66;03m# sublayer:previous layer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"take x and combine with output of next layer\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[12], line 31\u001b[0m, in \u001b[0;36mDecoderBlock.forward.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention_block(x, x, x, tgt_mask))  \u001b[38;5;66;03m# tgt_mask:becasue its decoder\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# ii. cross attention: second residual connection\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m1\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_connections[\u001b[38;5;241m2\u001b[39m](x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_block)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/noManEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 43\u001b[0m, in \u001b[0;36mMultiHeadAttentionBlock.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     41\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwq(q)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq::::::::::::::::::::::::::::::::::::::::::::::::::::\u001b[39m\u001b[38;5;124m\"\u001b[39m, q\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk::::::::::::::::::::::::::::::::::::::::::::::::::::\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv::::::::::::::::::::::::::::::::::::::::::::::::::::\u001b[39m\u001b[38;5;124m\"\u001b[39m, v\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     46\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk(k)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# from config import get_weights_file_path, get_config\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "def train_model(config):\n",
    "    # define device\n",
    "    device =  torch.device('cuda' if( torch.cuda.is_available) else 'cpu')\n",
    "    # device =torch.device('cpu')\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # making sure weight folder is created\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard: to visualize loss\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "    # optimize\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    \n",
    "    # resume training incase model crashes... restores stateof model and optimizer\n",
    "    initial_epoch=0\n",
    "    global_step=0\n",
    "    if(config['preload']):\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f\"Preloading model: {model_filename}\")\n",
    "        state = torch.load(model_filename)\n",
    "        inital_epoch = state['epoch']+1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step=state['global_step']\n",
    "        \n",
    "    # loss fn \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1)\n",
    "    # ignore_index: To ignore padding tokens so that they don't have any impact on calculating loss\n",
    "    # Label smoothing is a technique used to smooth the target labels by assigning a small probability to the incorrect classes and reducing the confidence on the correct class. \n",
    "    # This helps prevent the model from becoming too confident and overfitting to the training data. \n",
    "    # label_smoothing=0.1 means that for each true label, 10% of the probability mass is redistributed to all other classes.\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        model.train() # model.train() tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during \n",
    "        # training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch: {epoch:02d}')\n",
    "        for batch in batch_iterator:\n",
    "            encoder_input = batch['encoder_input'].to(device) # (batch_size, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (batch_size, seq_len)\n",
    "            encoder_mask= batch['encoder_mask'].to(device) # (batch_size,1,1, seq_len)\n",
    "            decoder_mask= batch['decoder_mask'].to(device) # (batch_size, 1 ,seq_len, seq_len)\n",
    "            \n",
    "            # run tensors through transformers\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask ) # (batch, seq_len, d_model)\n",
    "            print(\"decoderoutput###################################\")\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (batch, seq_len, d_model)\n",
    "            proj_ouput = model.project(decoder_output) # (batch, seq_len, tgt_vocab_size)\n",
    "            \n",
    "            # compare output with our labels\n",
    "            label = batch['label'].to(device)#(B, seq_len)\n",
    "            # (Batch, seq_len, tgt_vocab_size)  -->   (batch * seq_len, tgt_vocab_size)\n",
    "            loss = loss_fn(proj_ouput.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1)) \n",
    "            # update progress bar\n",
    "            batch_iterator.set_prefix(f\"loss: {loss.item():6.3f}\")\n",
    "            # log the loss\n",
    "            writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "            \n",
    "            # backpropagate\n",
    "            loss.backward()\n",
    "            \n",
    "            # update weihts\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step+=1  # this is basically or tensorboard\n",
    "            \n",
    "        # save the model at end of every epoch\n",
    "        \n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        #  it is good ide to save state of model + optimizer\n",
    "        torch.save({\n",
    "            'epoch':epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'global_step':global_step\n",
    "            }, model_filename)\n",
    "            \n",
    "if __name__ =='__main__':\n",
    "    warnings.filterwarnings('ignore')\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loss: 23123234234.000'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num=23123234234\n",
    "f\"loss: {num:6.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3,5).view(-1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3,5).view(10,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cls_token_id \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m sep_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cls_token_id, sep_token_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noManEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
